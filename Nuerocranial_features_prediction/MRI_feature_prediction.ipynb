{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af5cf3d-9fc4-4685-97ad-36c4b106d831",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_folder = \"./datasets/TMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b46d2-440a-4fda-9d34-1ae98caf817d",
   "metadata": {},
   "source": [
    "## 3DMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96132a-0423-46b9-bac0-f12a993f502b",
   "metadata": {},
   "source": [
    "### Original model including everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbc6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "models_folder = media_folder+\"/3DMM/UHM_models/\"\n",
    "currnet_model = 'UHM' \n",
    "\n",
    "# UHM model with all the components fused together (i.e. ears, inner mouth, and teeth)\n",
    "model_name = 'head_model_global_align'\n",
    "\n",
    "model_file = open(models_folder + model_name + '.pkl', 'rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning coordinates system to be in millimeter units\n",
    "scale_factor = 100\n",
    "\n",
    "# get model parameteres\n",
    "mean_shape = scale_factor*model_dict['Mean']\n",
    "mean_shape_CCS = mean_shape.reshape(-1,3)\n",
    "eigen_vec = model_dict['Eigenvectors']\n",
    "eigen_vec_num =  model_dict['Eigenvectors'].shape[1]\n",
    "eigen_val = model_dict['EigenValues']\n",
    "trilist = model_dict['Trilist']\n",
    "vertices_num = model_dict['Number_of_vertices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules (landmarks and masks)\n",
    "modules_folder = models_folder + '/Landmarks and masks/'\n",
    "\n",
    "modules_to_load = ['68_land_idxs'] # EEG_10_20_full_model / '49_plus_ears_land_idxs' / '68_land_idxs'\n",
    "\n",
    "landmarks = []\n",
    "landmarks_names = []\n",
    "landmarks_groups = []\n",
    "\n",
    "for currnet_module_name in modules_to_load:\n",
    "    module_file = open(modules_folder + currnet_module_name + '.pkl', 'rb')\n",
    "    currnet_module = pickle.load(module_file)\n",
    "    module_file.close()\n",
    "    if currnet_module_name=='EEG_10_20':\n",
    "        currnet_module_names = list(currnet_module.keys())\n",
    "        currnet_module = np.asarray(list(currnet_module.values()))\n",
    "    else:\n",
    "        currnet_module_names = list(map(str, 1+np.arange(len(currnet_module))))\n",
    "        \n",
    "    landmarks.append(currnet_module)\n",
    "    landmarks_names.append(currnet_module_names)\n",
    "    landmarks_groups.append(np.arange(len(currnet_module)))\n",
    "\n",
    "# turn list of lists into one list\n",
    "landmarks = [item for items in landmarks for item in items]\n",
    "landmarks_names = [item for items in landmarks_names for item in items]\n",
    "\n",
    "num_of_landmarks = len(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c3060-25cf-497f-9921-22b7b35ff714",
   "metadata": {},
   "source": [
    "### Lighter model including everything but eyes, teeth and inner mouth cavity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6328036-ddc5-43cf-b6e4-b2796579a647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "light_models_folder = media_folder+\"/3DMM/UHM_models/\"\n",
    "light_currnet_model = 'UHM' \n",
    "\n",
    "# UHM model with all the components fused together (i.e. ears, inner mouth, and teeth)\n",
    "light_model_name = 'head_model_global_align_no_mouth_and_eyes'\n",
    "\n",
    "light_model_file = open(light_models_folder + light_model_name + '.pkl', 'rb')\n",
    "light_model_dict = pickle.load(light_model_file)\n",
    "light_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3054a7-0bf9-48cf-b1a7-99fa02f11efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning cartesian coordinates system to be in millimeter units\n",
    "light_scale_factor = 100\n",
    "\n",
    "# get model parameteres\n",
    "light_mean_shape = light_scale_factor*light_model_dict['Mean']\n",
    "light_mean_shape_CCS = light_mean_shape.reshape(-1,3)\n",
    "light_eigen_vec = light_model_dict['Eigenvectors']\n",
    "light_eigen_vec_num =  light_model_dict['Eigenvectors'].shape[1]\n",
    "light_eigen_val = light_model_dict['EigenValues']\n",
    "light_trilist = light_model_dict['Trilist']\n",
    "light_vertices_num = light_model_dict['Number_of_vertices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe3562-7d6f-4ef4-b44e-7bd24289db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules (landmarks and masks)\n",
    "modules_folder = models_folder + '/Landmarks and masks/'\n",
    "\n",
    "modules_to_load = ['EEG_10_20'] # EEG_10_20 / '49_plus_ears_land_idxs' / '68_land_idxs'\n",
    "\n",
    "light_landmarks = []\n",
    "light_landmarks_names = []\n",
    "light_landmarks_groups = []\n",
    "\n",
    "for currnet_module_name in modules_to_load:\n",
    "    module_file = open(modules_folder + currnet_module_name + '.pkl', 'rb')\n",
    "    currnet_module = pickle.load(module_file)\n",
    "    module_file.close()\n",
    "    if currnet_module_name=='EEG_10_20':\n",
    "        currnet_module_names = list(currnet_module.keys())\n",
    "        currnet_module = np.asarray(list(currnet_module.values()))\n",
    "    else:\n",
    "        currnet_module_names = list(map(str, 1+np.arange(len(currnet_module))))\n",
    "        \n",
    "    light_landmarks.append(currnet_module)\n",
    "    light_landmarks_names.append(currnet_module_names)\n",
    "    light_landmarks_groups.append(np.arange(len(currnet_module)))\n",
    "\n",
    "# turn list of lists into one list\n",
    "light_landmarks = [item for items in light_landmarks for item in items]\n",
    "light_landmarks_names = [item for items in light_landmarks_names for item in items]\n",
    "\n",
    "num_of_light_landmarks = len(light_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a808d45e-3814-475b-a234-717e46e54495",
   "metadata": {},
   "source": [
    "### Matching landmark indices between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcca4c0-7085-49fe-a635-c5f4c42646cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_facial_landmarks = landmarks\n",
    "\n",
    "for current_landmark_index, current_landmark_vertex in enumerate(landmarks):\n",
    "    original_model_coordinates = mean_shape_CCS[current_landmark_vertex]\n",
    "    new_model_vertex_diffs = np.linalg.norm(light_mean_shape_CCS-original_model_coordinates, axis=1)\n",
    "    #sorted_diffs = np.sort(new_model_vertex_diffs)\n",
    "    #print((sorted_diffs[1]-sorted_diffs[0])/sorted_diffs[0])\n",
    "    light_facial_landmarks[current_landmark_index] = np.argmin(new_model_vertex_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa9226-4fb9-4b7e-8990-84bd8887c7a6",
   "metadata": {},
   "source": [
    "### Model Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea98343-06c2-4125-a88d-d9fbbfd6fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_light_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843fff2-d985-4324-8261-f63d12ea234c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if choose_light_model:\n",
    "    landmarks = np.concatenate((light_landmarks, light_facial_landmarks))\n",
    "    landmarks_names = list(np.concatenate((light_landmarks_names, landmarks_names)))\n",
    "    mean_shape = light_mean_shape\n",
    "    mean_shape_CCS = light_mean_shape_CCS\n",
    "    eigen_vec = light_eigen_vec\n",
    "    eigen_vec_num =  light_eigen_vec_num\n",
    "    eigen_val = light_eigen_val\n",
    "    trilist = light_trilist\n",
    "    vertices_num = light_vertices_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949313f1-06d2-49d4-8993-611504a35125",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b17bbc-ceaf-439f-b5f6-777bd18b7b5c",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd33abf-12cd-472f-a802-af461e94573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfad12-7777-41e0-9a8e-05b8334af447",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_landmarks_names = np.array([37, 40, 43, 46, 49, 55, 31, 9])\n",
    "rigid_facial_landmarks_names = np.array([37, 40, 43, 46, 28, 1, 17])\n",
    "\n",
    "center_of_the_eyebrows = np.array([20, 25])\n",
    "corners_of_the_eyebrows = np.array([18, 22, 23, 27])\n",
    "corners_of_the_eyes = np.array([37, 40, 43, 46])\n",
    "sides_of_the_face = np.array([1, 17])\n",
    "nose_bone = np.array([28, 31])\n",
    "lower_nose = np.array([32, 34, 36])\n",
    "corners_of_the_mouth = np.array([49, 55])\n",
    "chin = np.array([9])\n",
    "\n",
    "facial_landmarks = np.concatenate((center_of_the_eyebrows, corners_of_the_eyebrows, corners_of_the_eyes, sides_of_the_face, nose_bone, lower_nose))\n",
    "                                   #,corners_of_the_mouth, chin))\n",
    "selected_facial_indices = np.sort(facial_landmarks+num_of_light_landmarks-1)\n",
    "\n",
    "selected_EEG_10_20_landmark_names = light_landmarks_names\n",
    "selected_EEG_10_20_indices = []\n",
    "for current_index, current_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    selected_EEG_10_20_indices.append(landmarks_names.index(current_landmark_name))\n",
    "selected_EEG_10_20_indices = np.asarray(selected_EEG_10_20_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1843b-9990-4b81-b9af-4608d8004e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MRI_facial_landmarks = ['1', '2', '3', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30',\n",
    "                        '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48']\n",
    "MRI_facial_landmarks = [int(current_landmark) for current_landmark in MRI_facial_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92549f2c-63c7-47a2-8634-4d6344e5c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.concatenate((selected_facial_indices, selected_EEG_10_20_indices))\n",
    "selected_indices_names = np.take(landmarks_names, selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fcc0b-9003-4a90-9cd3-867b917f447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {}\n",
    "feature_sets['eye corners & eyebrow corners'] = np.concatenate((corners_of_the_eyebrows, corners_of_the_eyes))\n",
    "feature_sets['eye corners & eyebrow centers'] = np.concatenate((center_of_the_eyebrows, corners_of_the_eyes))\n",
    "feature_sets['eye corners & eyebrow corners and center'] = np.concatenate((center_of_the_eyebrows, corners_of_the_eyebrows, corners_of_the_eyes))\n",
    "feature_sets['eye corners & nose bone'] = np.concatenate((corners_of_the_eyes, nose_bone))\n",
    "feature_sets['nose bone & lower nose'] = np.concatenate((nose_bone, lower_nose))\n",
    "feature_sets['MRI_facial_landmarks'] = np.array(MRI_facial_landmarks)\n",
    "\n",
    "for current_key in feature_sets:\n",
    "    feature_sets[current_key] = feature_sets[current_key]+num_of_light_landmarks-1\n",
    "    feature_sets[current_key] = list(map(str, feature_sets[current_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45cab4-a8b9-4cab-804b-b8a24c7885f8",
   "metadata": {},
   "source": [
    "## Pytorch models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870801b3-75ce-456d-9eb3-17c1ff08ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b9204-759d-45c9-88c7-6d4828fc4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_3(nn.Module):\n",
    "    def __init__(self, inputs_size, output_size, first_hidden_layer_size=2**7, second_hidden_layer_size=2**5, third_hidden_layer_size=2**3):\n",
    "        super().__init__()\n",
    "        #self.layers = nn.Sequential(\n",
    "        self.inputs_size = inputs_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(inputs_size, first_hidden_layer_size)\n",
    "        self.bn1 = nn.BatchNorm1d(first_hidden_layer_size)\n",
    "        self.d1 = nn.Dropout(p=0.3, inplace=False)\n",
    "        \n",
    "        self.fc2 = nn.Linear(first_hidden_layer_size, second_hidden_layer_size)\n",
    "        self.bn2 = nn.BatchNorm1d(second_hidden_layer_size)\n",
    "        self.d2 = nn.Dropout(p=0.2, inplace=False)\n",
    "        \n",
    "        self.fc3 = nn.Linear(second_hidden_layer_size, third_hidden_layer_size)\n",
    "        self.bn3 = nn.BatchNorm1d(third_hidden_layer_size)\n",
    "        self.d3 = nn.Dropout(p=0.2, inplace=False)\n",
    "        \n",
    "        self.fc4 = nn.Linear(third_hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x) #torch.tanh(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x) #torch.tanh(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x) #torch.tanh(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15598741-d855-46bb-88db-fa03559d119a",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001979a4-8cf9-4823-aaa0-f05082b30a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_recent_trained_files(trained_folder_path, desired_feature_name, fold_index=-1):\n",
    "    trained_folder_and_features_set_files = trained_folder_path\n",
    "    trained_folder_files = os.listdir(trained_folder_and_features_set_files)\n",
    "\n",
    "    potential_files_creation_time = []\n",
    "    potential_files_index = []\n",
    "    \n",
    "    for i, current_filename in enumerate(trained_folder_files):\n",
    "        #print(current_filename)\n",
    "        if desired_feature_name in current_filename and 'model' in current_filename:# and desired_decimation_percentage in current_filename:\n",
    "            if fold_index != -1:# and len(current_filename)>30:\n",
    "                if fold_index==int(current_filename[-7]):\n",
    "                    current_filename_creation_time = current_filename[:17]\n",
    "                    potential_files_index.append(i)\n",
    "                    potential_files_creation_time.append(current_filename_creation_time)\n",
    "            else:\n",
    "                current_filename_creation_time = current_filename[:17]\n",
    "                potential_files_index.append(i)\n",
    "                potential_files_creation_time.append(current_filename_creation_time)\n",
    "    \n",
    "    most_recent_trained_model_filename = trained_folder_files[potential_files_index[np.argsort(potential_files_creation_time)[-1]]]\n",
    "    print(most_recent_trained_model_filename)\n",
    "    most_recent_trained_scaler_filename = most_recent_trained_model_filename[:-6]+'_scaler.pkl'\n",
    "    most_recent_trained_documentation_filename = most_recent_trained_model_filename[:-6]+'_documentation.txt'\n",
    "    \n",
    "    most_recent_trained_files = [most_recent_trained_model_filename, most_recent_trained_scaler_filename, most_recent_trained_documentation_filename]\n",
    "    \n",
    "    return most_recent_trained_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb502844-78b7-4648-b8cf-756183fcf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, dataloader, loss_function, optimizer, output_size, mode):\n",
    "    current_loss = 0.0\n",
    "    if mode=='train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "                \n",
    "        targets = targets.reshape((targets.shape[0], output_size))\n",
    "        if mode=='train':\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "        # Perform forward pass\n",
    "        current_outputs = model(inputs)\n",
    "        # Compute loss\n",
    "        loss = loss_function(current_outputs, targets)\n",
    "        if mode=='test':\n",
    "            if i==0:\n",
    "                outputs = current_outputs\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, current_outputs), dim=0)\n",
    "        if mode=='train':\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "        #if mode!='test':\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "    if mode=='test':\n",
    "        return current_loss, model, outputs\n",
    "    else:\n",
    "        return current_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b0ab0-8e66-421a-b3b8-c2d4052e9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=500):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    validation_losses = []\n",
    "    test_losses = []\n",
    "    validation = False\n",
    "    batch_size = 2**3\n",
    "    \n",
    "    if mode == 'train':\n",
    "        lr = 5e-3\n",
    "    elif mode == 'fine_tune':\n",
    "        lr = 2.5e-3\n",
    "    else:\n",
    "        lr = 0\n",
    "\n",
    "    loss_function = nn.MSELoss() #nn.L1Loss\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "    \n",
    "    if y_test.shape[1]>1:\n",
    "        output_size = 3\n",
    "    else:\n",
    "        output_size = 1\n",
    "    \n",
    "    if mode != 'test_only':\n",
    "        validation = False # False / True\n",
    "        if np.array_equal(y_test, np.array([-1, -1, -1]).reshape(1, 3)): #only training on the entire dataset\n",
    "            validation = False\n",
    "        triggered = False\n",
    "\n",
    "        train_input = torch.tensor(X_train.astype(np.float32))\n",
    "        train_target = torch.tensor(y_train.astype(np.float32)) \n",
    "\n",
    "        train_tensor = TensorDataset(train_input, train_target)\n",
    "\n",
    "        if validation==True:\n",
    "            train_set_percentage = 0.8\n",
    "            last_validation_loss = 1e6\n",
    "            lowest_validation_loss = 1e6\n",
    "            patience = 4\n",
    "            #tolerance = 0.001*1e-3\n",
    "            trigger_times = 0\n",
    "\n",
    "            train_set_size = int(np.round(train_set_percentage*train_input.shape[0]))\n",
    "            valid_set_size = train_input.shape[0]-train_set_size\n",
    "            \n",
    "            if train_set_size%batch_size==1 and valid_set_size%batch_size!=0:\n",
    "                train_set_size -= 1\n",
    "                valid_set_size += 1\n",
    "            elif valid_set_size%batch_size==1 and train_set_size%batch_size!=0:\n",
    "                train_set_size += 1\n",
    "                valid_set_size -= 1\n",
    "            elif train_set_size%batch_size==1:\n",
    "                train_set_size -= 1\n",
    "                train_tensor = TensorDataset(train_input[:-1, :], train_target[:-1, :])\n",
    "            elif valid_set_size%batch_size==1:\n",
    "                valid_set_size -= 1\n",
    "                train_tensor = TensorDataset(train_input[:-1, :], train_target[:-1, :])\n",
    "\n",
    "            train_tensor, valid_tensor = random_split(train_tensor, [train_set_size, valid_set_size])\n",
    "\n",
    "            validloader = DataLoader(dataset=valid_tensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        trainloader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    if np.array_equal(y_test, np.array([-1, -1, -1]).reshape(1, 3))==False:\n",
    "        test_input = torch.tensor(X_test.astype(np.float32))\n",
    "        test_target = torch.tensor(y_test.astype(np.float32))\n",
    "\n",
    "        test_tensor = TensorDataset(test_input, test_target) \n",
    "        testloader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Run the training loop\n",
    "    for epoch in range(num_of_epochs):\n",
    "        if mode != 'test_only':\n",
    "            if triggered==True:\n",
    "                continue\n",
    "\n",
    "            train_loss, mlp = run_model(mlp.to(device), trainloader, loss_function, optimizer, output_size, mode='train')\n",
    "\n",
    "            if validation==True:\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, _ = run_model(mlp.to(device), validloader, loss_function, optimizer, output_size, mode='validation')\n",
    "                    validation_losses.append(validation_loss)\n",
    "\n",
    "                    last_validation_loss = validation_loss\n",
    "                    if validation_loss<lowest_validation_loss:\n",
    "                        lowest_validation_loss=validation_loss\n",
    "                        lowest_validation_loss_model=copy.deepcopy(mlp)\n",
    "                        lowest_validation_loss_epoch=epoch\n",
    "\n",
    "                    if last_validation_loss < validation_loss and epoch>50:\n",
    "                        trigger_times += 1\n",
    "                        if trigger_times > patience:\n",
    "                            triggered = True\n",
    "                            print(('Early stopping at epoch '+str(epoch)))\n",
    "                    else:\n",
    "                        trigger_times = 0\n",
    "\n",
    "            test_loss , _, _ = run_model(mlp.to(device), testloader, loss_function, optimizer, output_size, mode='test')\n",
    "            scheduler.step()\n",
    "\n",
    "        # Disable grad\n",
    "        if np.array_equal(y_test, np.array([-1, -1, -1]).reshape(1, 3))==False:\n",
    "            with torch.no_grad():\n",
    "                test_loss , _, predictions = run_model(mlp.to(device), testloader, loss_function, optimizer, output_size, mode='test')\n",
    "                test_losses.append(test_loss)\n",
    "                #print(epoch, test_loss)\n",
    "\n",
    "            \n",
    "    if mode != 'test_only' and np.array_equal(y_test, np.array([-1, -1, -1]).reshape(1, 3))==False:\n",
    "        lowest_validation_loss_epoch=num_of_epochs-1\n",
    "        lowest_validation_loss_model=copy.deepcopy(mlp)\n",
    "        print(f\"lowest_validation_loss_epoch is {num_of_epochs-1}\")\n",
    "\n",
    "        if validation==True:\n",
    "            with torch.no_grad():\n",
    "                _ , _, predictions = run_model(lowest_validation_loss_model.to(device), testloader, loss_function, optimizer, output_size, mode='test')\n",
    "    \n",
    "    if mode != 'test_only' and np.array_equal(y_test, np.array([-1, -1, -1]).reshape(1, 3))==False:#validation==True:\n",
    "        return predictions, validation_losses, test_losses, lowest_validation_loss_model, lowest_validation_loss_epoch\n",
    "    elif np.array_equal(y_test, np.array([-1, -1, -1]).reshape(1, 3)):\n",
    "        return mlp\n",
    "    else:\n",
    "        return predictions, validation_losses, test_losses, 'a', 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463119c-8437-480f-9d2c-0171ea51ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_original_space(prediction, subject_id):\n",
    "    number_of_transformations=5\n",
    "    coordinates = []\n",
    "    coordinates.append(prediction)\n",
    "    for i in range(number_of_transformations):\n",
    "        current_transformation_matrix = inverse_transformations_df.iloc[(number_of_transformations-1-i)*4:(number_of_transformations-i)*4, np.arange(4*subject_id, 4*(subject_id+1))].values\n",
    "        current_extended_coordinates = np.concatenate((np.array(coordinates[i]).reshape(1, coordinates[i].shape[0]), np.ones((1, 1))), axis=1).T\n",
    "        current_product = current_transformation_matrix@current_extended_coordinates\n",
    "        coordinates.append(current_product[:3])\n",
    "    return coordinates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0d4c9-2b0d-48ea-8ae1-59829c3c4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_distances(plane_normal, plane_point, other_point):\n",
    "    plane_normal = plane_normal/np.linalg.norm(plane_normal)\n",
    "    distance_vector = other_point-plane_point\n",
    "    perpendicular_distance = np.dot(distance_vector, plane_normal)\n",
    "    plane_projected_other_point = other_point - perpendicular_distance*plane_normal\n",
    "    tangent_distance = np.linalg.norm(plane_projected_other_point-plane_point)\n",
    "    return perpendicular_distance, tangent_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780778a-7c5a-4392-86ee-f39a2146c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_keys(OrderedDict):\n",
    "    newOrderedDict = OrderedDict.copy()\n",
    "    for i, key in enumerate(OrderedDict):\n",
    "        key, value = newOrderedDict.popitem(False)\n",
    "        if 'module.' in key:\n",
    "            key = key.replace('module.', '')\n",
    "            newOrderedDict[key] = value\n",
    "        else:\n",
    "            newOrderedDict[key] = value\n",
    "    return newOrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c378a16-7d13-4b77-9a72-dd1adb27b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_saver(fig, filename):\n",
    "    \n",
    "    pio.kaleido.scope.mathjax = None\n",
    "\n",
    "    timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "    figure_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/\"\n",
    "    figure_filename = f\"{filename}_{timestamp_string}\" #???\"experiment_\"+str(experiment_number)+\"_\"+timestamp_string+\"_\"+filename\n",
    "    figure_filetype = \".eps\"\n",
    "\n",
    "    figure_path = figure_folder + figure_filename + figure_filetype\n",
    "\n",
    "    fig.layout.title = \"\"\n",
    "    \n",
    "    fig.layout.margin.t = 0#0.075*fig.layout.height\n",
    "    fig.layout.margin.b = 0\n",
    "    fig.layout.margin.l = 0\n",
    "    fig.layout.margin.r = 10#0.05*fig.layout.width\n",
    "\n",
    "    pio.write_image(fig, figure_path)#, scale=5)\n",
    "    pio.write_json(fig, file=figure_folder+figure_filename+\".json\" ,validate=True, pretty=True, remove_uids=False, engine='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f931139-d461-4757-9868-0dd28a95fcb0",
   "metadata": {},
   "source": [
    "# IXI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f666c-6037-478f-bc68-2f5963742db1",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81e438-c1bd-4196-bb17-e354368c25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_scale_factor = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed57082-e503-4aeb-b8a1-718301d773c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = media_folder+\"/MRI_datasets/\"\n",
    "current_dataset_name = 'IXI'\n",
    "dataset_filename = 'Dataset_Chamfer.xlsx' #'Dataset.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6f7ab-a734-49ab-a306-0b088306aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_subject_dataframe = pd.ExcelFile(datasets_folder+current_dataset_name+'/'+dataset_filename)\n",
    "current_sheet_names = current_subject_dataframe.sheet_names\n",
    "current_num_of_sheets = len(current_sheet_names)\n",
    "\n",
    "skin_coordinates_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Skin coordinates')\n",
    "skin_normals_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Skin normals')\n",
    "skin_geodesic_distances_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Skin distances')\n",
    "inverse_matrices_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Inverse transformations')\n",
    "stats_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf1cf9-c1ac-4f06-a6af-8ac08a3293b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_coordinates_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=skin_coordinates_index, index_col=0)\n",
    "skin_normals_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=skin_normals_index, index_col=0)\n",
    "skin_geodesic_distances_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=skin_geodesic_distances_index, index_col=0)\n",
    "inverse_transformations_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=inverse_matrices_index, index_col=0)\n",
    "stats_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=stats_index, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2cabd-f66b-4a1a-b496-48aa3a205298",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_coordinates_columns_names = list(skin_coordinates_df.columns)\n",
    "only_coordinates_columns_indices = []\n",
    "\n",
    "for i in range(len(skin_coordinates_columns_names)):\n",
    "    if 'indices' not in skin_coordinates_columns_names[i]:\n",
    "        only_coordinates_columns_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b762a3-f2a4-45c6-bd9d-cdf98178e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    max_euclidean_distance = 75e-3 # that's a lot\n",
    "\n",
    "    relevant_indices = []\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(skin_coordinates_df.index[:num_of_light_landmarks]):\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = valid_coordinates_rows#np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "\n",
    "        desired_landmark_coordinates_mean = np.mean(desired_landmark_subjects_coordinates[valid_rows, :], axis=0)\n",
    "        euclidean_distances = np.linalg.norm(desired_landmark_subjects_coordinates[valid_rows, :]-desired_landmark_coordinates_mean, axis=1)\n",
    "        desired_landmark_relevant_indices = np.where(euclidean_distances<max_euclidean_distance)[0]\n",
    "        relevant_indices.append(desired_landmark_relevant_indices)\n",
    "\n",
    "    only_valid_score_subjects_rows = relevant_indices[0]\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(skin_coordinates_df.index[:num_of_light_landmarks]):\n",
    "        only_valid_score_subjects_rows = np.intersect1d(relevant_indices[desired_landmark_index], only_valid_score_subjects_rows)\n",
    "else:\n",
    "    score_ratio_threshold = 1\n",
    "    only_valid_score_subjects_rows = np.sort(np.argsort(stats_df.loc['unique_correspondence_final_loss', :].values)[:int(score_ratio_threshold*stats_df.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a206f-4efd-4972-bec5-e08da8ecc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    array_folder = datasets_folder+current_dataset_name+'/'\n",
    "    array_filename = 'chamfer_distance_subjects_names'\n",
    "    array_filetype = '.npy'\n",
    "\n",
    "    array_path = array_folder + array_filename + array_filetype\n",
    "    \n",
    "    if 0:\n",
    "        with open(array_path, 'wb') as file:\n",
    "            np.save(file, only_valid_score_subject_names)\n",
    "    else:\n",
    "        with open(array_path, 'rb') as file:\n",
    "            only_valid_score_subject_names = np.load(file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59547cd-6e1e-4d59-8e60-439411561fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subject_names = np.array(skin_coordinates_df.columns[0::4])\n",
    "only_valid_score_subjects_rows = []\n",
    "for current_name in only_valid_score_subject_names:\n",
    "    only_valid_score_subjects_rows.append(np.where(all_subject_names==f'{current_name}_indices')[0][0])\n",
    "    \n",
    "only_valid_score_subjects_rows = np.array(only_valid_score_subjects_rows)\n",
    "only_valid_score_subject_names = [current_subject_name[:-8] for current_subject_name in all_subject_names[only_valid_score_subjects_rows]]\n",
    "only_valid_score_subject_names = np.array(only_valid_score_subject_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1227d1e-ff0a-4d66-8173-6de069313a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df84815-c1b2-47e3-932b-7caab10ed343",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "ablation_study = False\n",
    "MLP_nn = MLP_3\n",
    "MLP_folder = 'MLP_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d38346-c2ad-4664-8720-55126b137cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ablation_study==False:\n",
    "    X_perturbations = np.zeros((only_valid_score_subjects_rows.size, 3*len(MRI_facial_landmarks)))\n",
    "else:\n",
    "    added_facial_landmarks_noise_norm = 1 #mm\n",
    "    \n",
    "    unnormalized_perturbations = np.random.normal(loc=0, scale=1, size=(int(only_valid_score_subjects_rows.size*len(MRI_facial_landmarks)), 3))\n",
    "    unnormalized_perturbations_magnitudes = np.linalg.norm(unnormalized_perturbations, axis=1)\n",
    "    normalized_perturbations = unnormalized_perturbations/unnormalized_perturbations_magnitudes.reshape(-1, 1)\n",
    "\n",
    "    perturbation_magnitudes = np.random.normal(loc=added_facial_landmarks_noise_norm, scale=0.5, size=int(only_valid_score_subjects_rows.size*len(MRI_facial_landmarks))).reshape(-1, 1)    \n",
    "    \n",
    "    perturbations = normalized_perturbations*1*registration_scale_factor*perturbation_magnitudes\n",
    "    \n",
    "    X_perturbations = perturbations.reshape(-1, len(MRI_facial_landmarks)*3)\n",
    "\n",
    "X_perturbations = np.concatenate((X_perturbations, np.zeros((only_valid_score_subjects_rows.size, 3))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296a40b-c177-4d8d-a6ed-7ef4e1776320",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "kfold_train_indices = []\n",
    "kfold_test_indices = []\n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(kfold.split(np.arange(only_valid_score_subjects_rows.size))):\n",
    "    kfold_train_indices.append(train_indices)\n",
    "    kfold_test_indices.append(test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936bc57-9e57-4165-a058-09f52319fd48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Euclidean coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58a6a2-eb3a-458c-88de-8b200ce83cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder_path = media_folder+\"/3DMM/Trained_models/pytorch_MLP/Coordinates/\"+MLP_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e78c77-e1b3-4590-93bc-0b0be3e8cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee91294-b0b5-4ff0-99c2-a59d6598f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features_set_name = 'MRI_facial_landmarks'\n",
    "#chosen_features_set_index = [idx for idx, key in enumerate(list(feature_sets.items()) ) if key[0] == chosen_features_set_name][0]\n",
    "desired_decimation_percentage = str(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a1bf0-dfed-4e3c-bb16-210609ac089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_MAE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_MAE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "IXI_Euclidean_MSE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_MSE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "IXI_Euclidean_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_std_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "IXI_Euclidean_perpendicular_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_tangent_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_distances_ratios = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_distances_ratios_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a5651-e3eb-4558-bffe-a32a89504f72",
   "metadata": {},
   "source": [
    "#### Synthetic model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895eed7-c18c-4ece-9aac-b7c34d9ef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"3DMM/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"3DMM/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = joblib.load(trained_folder_path+\"3DMM/\"+scaler_filename) \n",
    "        #X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_test.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"3DMM/\"+model_filename)))\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "        #validation_losses.append(validation_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        \n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b6c5f-26d4-49af-89be-823bcc183502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e749ba3-734f-487f-ba8f-47d07ef746ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8b930-2696-4daa-a8cf-cf411ec2fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_MAE[0, :] = mean_errors\n",
    "IXI_Euclidean_MAE_mean[0, :] = mean_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fe7db-e01d-4089-a6bc-8f22ff95eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_MSE[0, :] = mean_squared_errors\n",
    "IXI_Euclidean_MSE_mean[0, :] = mean_squared_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd862889-5f6e-4ca4-95d4-9d095fd8f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_std[0, :] = std_errors\n",
    "IXI_Euclidean_std_mean[0, :] = std_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d837a2-e2f1-45af-b1bb-2d0195ba650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_perpendicular_distances[0, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_tangent_distances[0, :] = tangent_distances_mean\n",
    "IXI_Euclidean_distances_ratios[0, :] = distances_ratios_mean\n",
    "IXI_Euclidean_distances_ratios_std[0, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f2239-78e3-45d1-9a08-ec3e8cf5bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_MSE[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_distances_ratios[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a3521-0a19-41d6-8b14-c1811c808059",
   "metadata": {},
   "source": [
    "#### ADNI model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508bda7-c963-41c3-bcc2-84d6a6dc97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"ADNI_ALL/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"ADNI_ALL/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = joblib.load(trained_folder_path+\"ADNI_ALL/\"+scaler_filename) \n",
    "        #X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "        \n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"ADNI_ALL/\"+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ee130-f64e-4673-871e-e60cbdc35a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21140f-4c3e-42ad-bb82-5c54dc492bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334751a-bb83-409d-a9cf-41b42002d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_MSE[1, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fa332-3219-41fe-8c6f-6171369b9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_std[1, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fcc88c-8f64-43ef-9173-d71006f34173",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_perpendicular_distances[1, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_tangent_distances[1, :] = tangent_distances_mean\n",
    "IXI_Euclidean_distances_ratios[1, :] = distances_ratios_mean\n",
    "IXI_Euclidean_distances_ratios_std[1, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04398941-a4f2-4316-9ba9-7a9a3061a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'other_dataset'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_MSE[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_distances_ratios[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49c516-538f-4249-a6fc-8f16916b1568",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ee654-a0a3-4c44-8bd5-f760b5d8742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_IXI/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed278372-13e4-4eda-b279-34f8de8fff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        if train_now:\n",
    "            model_to_load_folder = \"3DMM/\"\n",
    "        else:\n",
    "            model_to_load_folder = \"3DMM_IXI/\"\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        if train_now:\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                #f\"landmark_names_being_used: {features_to_use}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b945778-13c3-4ebe-b7ae-f32dffd64d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb23037-dcf3-4c14-99fb-36c902647d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcabf9-1525-4549-9727-57eaedf4c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72874589-3b8d-4e37-a37a-d7c97004e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_MSE[2, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc72911-02fe-4ea2-82b0-465485ce444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_std[2, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b294de-0efc-485d-97ea-5522109efa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_perpendicular_distances[2, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_tangent_distances[2, :] = tangent_distances_mean\n",
    "IXI_Euclidean_distances_ratios[2, :] = distances_ratios_mean\n",
    "IXI_Euclidean_distances_ratios_std[2, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c445e2-4ba3-4177-b389-dd2494cbb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_MSE[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_distances_ratios[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392d5c9-e034-47b3-96e5-10eabbe74c2f",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa25c9-db40-4616-9062-f9e0bec2d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_IXI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac9eb0-f104-43be-a568-5754919410eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "    if train_now:\n",
    "        model_to_load_folder = \"3DMM/\"\n",
    "    else:\n",
    "        model_to_load_folder = \"3DMM_IXI_ALL/\"\n",
    "\n",
    "    most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "    model_filename = most_recent_trained_model_filenames[0]\n",
    "    scaler_filename = most_recent_trained_model_filenames[1]\n",
    "    documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "    with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "        documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "    desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "    desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "    desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "    if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "        valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "    else:\n",
    "        valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "    valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "    #__________________________________________________________________________________________________________\n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "    \n",
    "    #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "    features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "    features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "    features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "    X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "    for i in range(X_array.shape[0]):\n",
    "        X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "    X_array = X_array[valid_rows, :]\n",
    "\n",
    "    X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "    X_test = []\n",
    "    \n",
    "    X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "    X_train = X_imputer.fit_transform(X_train)\n",
    "    #X_test = X_imputer.transform(X_test)\n",
    "    \n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    #X_test = X_standard_scaler.transform(X_test)\n",
    "    \n",
    "    #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "    #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "    \n",
    "    mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "    mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "    if 1:\n",
    "        for i, param in enumerate(mlp.parameters()):\n",
    "            param.requires_grad=True\n",
    "\n",
    "    if train_now:\n",
    "        predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "        validation_losses.append(validation_loss)\n",
    "        #test_losses.append(test_loss)\n",
    "    else:\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "\n",
    "    if save_model and train_now:\n",
    "        timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "        torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "        pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "        documentation = [\n",
    "            f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "            #f\"landmark_names_being_used: {features_to_use}\",\n",
    "            f\"number_of_training_samples: {train_indices.size}\",\n",
    "            f\"model: {mlp}\",\n",
    "        ]\n",
    "        with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a258c-1d57-429e-abf3-9a784f8711b9",
   "metadata": {},
   "source": [
    "#### Learn from scratch - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7a2c0-7635-4b13-9e16-8648d6aa1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"IXI/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154feef-7bbe-4e01-9285-4786e14d7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"IXI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                #f\"landmark_names_being_used: {features_to_use}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965084a-4e6c-48c5-8499-d3e3098942a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ablation Study')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"IXI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                #f\"landmark_names_being_used: {features_to_use}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26705901-33d2-4d14-9467-99e8283b9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93cf3d-d0c7-4d84-84ec-a04884e2e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d064244-41a0-47fa-92e3-53b3da6b87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadf488-ffa3-4700-aa12-6e55ae1dace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff36122-1f85-4b48-a7b1-b81a43bc8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd90816-d295-471e-a026-8b62edea829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d2fb4-0cf7-4ab0-98c6-6c760e540d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f985cea-c008-48e2-ab60-86359c1a6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_MSE[3, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b80d64-fdd6-4275-ada8-169dca001172",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_std[3, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13dfac-4f48-4e04-a1e0-7876fce7fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_perpendicular_distances[3, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_tangent_distances[3, :] = tangent_distances_mean\n",
    "IXI_Euclidean_distances_ratios[3, :] = distances_ratios_mean\n",
    "IXI_Euclidean_distances_ratios_std[3, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47af6a1-4249-494f-8d34-c3000e503985",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_MSE[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_distances_ratios[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1693b2-33b5-490c-9e7b-f982a18e897f",
   "metadata": {},
   "source": [
    "#### Learn from scratch - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183af4f-3aba-4d78-85a9-3bc153a2feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"IXI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b383a-fc6e-42c3-abc8-aa81cfd5af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "    desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "    desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "    desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "    if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "        valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "    else:\n",
    "        valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "    valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "    #__________________________________________________________________________________________________________\n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "\n",
    "    #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "    features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "    features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "    features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "    X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "    for i in range(X_array.shape[0]):\n",
    "        X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "    X_array = X_array[valid_rows, :]\n",
    "\n",
    "    X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "    X_test = []\n",
    "\n",
    "    X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "    X_train = X_imputer.fit_transform(X_train)\n",
    "    #X_test = X_imputer.transform(X_test)\n",
    "\n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    #X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "    #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "    #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "    if train_now:\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "                \n",
    "        trained_model = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "        #test_losses.append(test_loss)\n",
    "\n",
    "    if save_model and train_now:\n",
    "        timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "        torch.save(trained_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "        pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "        documentation = [\n",
    "            f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "            #f\"landmark_names_being_used: {features_to_use}\",\n",
    "            f\"number_of_training_samples: {train_indices.size}\",\n",
    "            f\"model: {mlp}\",\n",
    "        ]\n",
    "        with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05035195-f23e-44d7-99a8-1b4f8fd76534",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Euclidean coordinates and Geodesic distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a495cd1-b9d1-48d2-b964-d9a0c2091aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder_path = media_folder+\"/3DMM/Trained_models/pytorch_MLP/Coordinates_Geodesic/\"+MLP_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75228bb-49dc-488c-b510-00a7da7dfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c1f39-e01c-4eeb-9275-deddf86de9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features_set_name = 'MRI_facial_landmarks'\n",
    "#chosen_features_set_index = [idx for idx, key in enumerate(list(feature_sets.items()) ) if key[0] == chosen_features_set_name][0]\n",
    "desired_decimation_percentage = str(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeea0f6-3e0c-4247-a60b-ec1b2ce961ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_MAE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_Geodesic_MAE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "IXI_Euclidean_Geodesic_MSE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_Geodesic_MSE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "IXI_Euclidean_Geodesic_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_Geodesic_std_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "IXI_Euclidean_Geodesic_perpendicular_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_Geodesic_tangent_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_Geodesic_distances_ratios = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "IXI_Euclidean_Geodesic_distances_ratios_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0cabf9-be79-4c09-96d2-0de8be6f28a1",
   "metadata": {},
   "source": [
    "#### Synthetic model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f62129-4064-4d45-82e9-968a71dd345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"3DMM/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"3DMM/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"3DMM/\"+model_filename)))\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "        #validation_losses.append(validation_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8283c63-991a-48f4-b338-a5d37a72390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a53d9-a896-4e9a-aa26-bdb20cf69140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd49f9-0909-49fe-bb35-d8d3c5392c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_MAE[0, :] = mean_errors\n",
    "IXI_Euclidean_Geodesic_MAE_mean[0, :] = mean_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136c78f-915e-47a5-84e3-57a9bbd51c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_MSE[0, :] = mean_squared_errors\n",
    "IXI_Euclidean_Geodesic_MSE_mean[0, :] = mean_squared_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3c868-3715-4d29-aa12-69ae92ca7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_std[0, :] = std_errors\n",
    "IXI_Euclidean_Geodesic_std_mean[0, :] = std_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f492c-ec12-4999-b25d-19843a9f08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_perpendicular_distances[0, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_Geodesic_tangent_distances[0, :] = tangent_distances_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios[0, :] = distances_ratios_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios_std[0, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289a873-7fb6-4dd2-9998-5e03f18f5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_MSE[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_distances_ratios[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2a636-41ae-47e0-b928-0b645306eaf6",
   "metadata": {},
   "source": [
    "#### ADNI model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c75ab-134a-4202-a649-4f0ecea25d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"ADNI_ALL/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"ADNI_ALL/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = joblib.load(trained_folder_path+\"ADNI_ALL/\"+scaler_filename) \n",
    "        #X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"ADNI_ALL/\"+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e8fe6-d453-4377-86a1-e0a75f4e1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddca7e8-3f9f-4cee-a97a-84a0665d562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f0b42-685b-404f-8539-c726e5972b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_MSE[1, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01d941-d8b9-4527-901f-090cd103972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_std[1, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d120ed8-1050-4415-aec5-dac511f3136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_perpendicular_distances[1, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_Geodesic_tangent_distances[1, :] = tangent_distances_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios[1, :] = distances_ratios_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios_std[1, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445fcb02-db5a-4741-aa05-6ac4c9cadc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'other_dataset_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_MSE[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_distances_ratios[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85a89e-4412-44a4-9e16-8fd8edcd4730",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb13fc-f90c-4e35-8a12-805236ebce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_IXI/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799acb6-7509-489c-aa62-4e9369bc7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        if train_now:\n",
    "            model_to_load_folder = \"3DMM/\"\n",
    "        else:\n",
    "            model_to_load_folder = \"3DMM_IXI/\"\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        if train_now:\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "            #validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e711684-8d2c-4316-9cf5-bc46e3e4f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea5875-b81c-4d86-a5df-fa8e175c18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d86b0f-e2f7-473a-877e-d8251f97d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afcbd6-55ee-476d-bb9f-671a85e5a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_MSE[2, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd88d3-7a7a-4eec-be1a-2939cb242493",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_std[2, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f403d-b949-4b90-9a65-d4388dd72877",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_perpendicular_distances[2, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_Geodesic_tangent_distances[2, :] = tangent_distances_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios[2, :] = distances_ratios_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios_std[2, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebcb83-87dc-4f3f-bfa6-012bf00fa96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_MSE[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_distances_ratios[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4174692-9a96-4ac5-8fc6-5ba4e235bd6c",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07bd4c-0aa5-49c2-b778-a576abb9afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_IXI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "\tif train_now:\n",
    "\t\tmodel_to_load_folder = \"3DMM/\"\n",
    "\telse:\n",
    "\t\tmodel_to_load_folder = \"3DMM_IXI_ALL/\"\n",
    "\n",
    "\tmost_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "\tmodel_filename = most_recent_trained_model_filenames[0]\n",
    "\tscaler_filename = most_recent_trained_model_filenames[1]\n",
    "\tdocumentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "\twith open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "\t\tdocumentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "\tdesired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "\tdesired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "\tdesired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "\tif np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "\t\tvalid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "\telse:\n",
    "\t\tvalid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "\tvalid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "\t#__________________________________________________________________________________________________________   \n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "\n",
    "\tcoordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "\tcoordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "\tcoordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "\tcoordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "\tfor i in range(coordinates_X_array.shape[0]):\n",
    "\t\tcoordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "\tcoordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "\tgeodesic_features_data = skin_geodesic_distances_df\n",
    "\tgeodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "\tfor i in range(geodesic_features_data.shape[1]):\n",
    "\t\tif geodesic_features_data.iloc[0, i]:\n",
    "\t\t\tgeodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "\t\telse:\n",
    "\t\t\tgeodesic_X_array[i, 0] = 0\n",
    "\n",
    "\t\tif geodesic_features_data.iloc[1, i]:\n",
    "\t\t\tgeodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "\t\telse:\n",
    "\t\t\tgeodesic_X_array[i, 1] = 0\n",
    "\n",
    "\t\tif np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "\t\t\tgeodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "\t\telse:\n",
    "\t\t\tgeodesic_X_array[i, 2] = 0\n",
    "\n",
    "\tgeodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "\tX_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "\tX_test = []\n",
    "\n",
    "\tX_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "\tX_train = X_imputer.fit_transform(X_train)\n",
    "\t#X_test = X_imputer.transform(X_test)\n",
    "\n",
    "\tX_standard_scaler = StandardScaler()\n",
    "\tX_train = X_standard_scaler.fit_transform(X_train)\n",
    "\t#X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "\t#print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "\t#print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "\tmlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\tmlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "\tif 1:\n",
    "\t\tfor i, param in enumerate(mlp.parameters()):\n",
    "\t\t\tparam.requires_grad=True\n",
    "\n",
    "\tif train_now:\n",
    "\t\tpredictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "\t\tvalidation_losses.append(validation_loss)\n",
    "\t\t#test_losses.append(test_loss)\n",
    "\telse:\n",
    "\t\tpredictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "\tif save_model and train_now:\n",
    "\t\ttimestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\t\ttimestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "\t\ttorch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "\t\tpickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "\t\tdocumentation = [\n",
    "\t\t\tf\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "\t\t\tf\"number_of_training_samples: {train_indices.size}\",\n",
    "\t\t\tf\"model: {mlp}\",\n",
    "\t\t]\n",
    "\t\twith open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "\t\t\ttxt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842ca35-b351-492f-9719-0b4d26b6d7e3",
   "metadata": {},
   "source": [
    "#### Learn from scratch - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252ac24-ecee-4bdd-a4dc-1e642c1bdc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"IXI/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd4ef9-7d03-4564-a531-97ca1999613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        #for i in range (len(coordinates_features_to_use)):\n",
    "        #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"IXI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5b9e2-0410-445c-849a-24087c06c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ablation Study')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        #for i in range (len(coordinates_features_to_use)):\n",
    "        #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"IXI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262505ce-8eae-43b3-8ff8-a64923d595d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        #for i in range (len(coordinates_features_to_use)):\n",
    "        #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"IXI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2cff8-c4fa-4dac-bc0f-b77081e132ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        #for i in range (len(coordinates_features_to_use)):\n",
    "        #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"IXI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f568c-fe7c-480f-ac13-f7d3ee176c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166514ed-0d05-4e05-b8a8-edeac4212759",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba862cfa-750f-47f8-9281-6cc06a888df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0efcef-1768-4403-8556-d0cbeac505db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af783d1d-fe7d-4e59-aafa-2b02caa69493",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3376146-fd0c-4487-802f-5f561c31a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE seed=4: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab0a1f-fba4-4984-a39f-55d7bd0594b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds seed=4: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a4625-6394-4496-a11a-6fdb30daf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE MLP2: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ca70e-3154-40a8-9ce0-afafb3610097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds MLP2: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f2be0-4d81-44c0-9d88-7a59ac0ff33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8330d-d406-4523-8a42-cd1de2d473a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329745fe-2f51-4001-8cec-4d957697f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_MSE[3, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d07fb-f06e-4a98-9b3b-e103b0deab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_std[3, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d468e-24ab-4d77-89af-62b9c5bbb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IXI_Euclidean_Geodesic_perpendicular_distances[3, :] = perpendicular_distances_mean\n",
    "IXI_Euclidean_Geodesic_tangent_distances[3, :] = tangent_distances_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios[3, :] = distances_ratios_mean\n",
    "IXI_Euclidean_Geodesic_distances_ratios_std[3, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e0baf-75f5-4444-98e2-eec6fffa3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_MSE[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = IXI_Euclidean_Geodesic_distances_ratios[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1b57c-6f93-44d2-ad81-cd4a8f0766c1",
   "metadata": {},
   "source": [
    "#### Learn from scratch - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6a4e6-e2b2-4aef-8522-bb2d1615d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"IXI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599d332-f6d4-4356-8d70-a259053cc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "    desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "    desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "    desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "    if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "        valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "    else:\n",
    "        valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "    valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "    #__________________________________________________________________________________________________________\n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "    #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "    #for i in range (len(coordinates_features_to_use)):\n",
    "    #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "    coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "    coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "    coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "    coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "    for i in range(coordinates_X_array.shape[0]):\n",
    "        coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "    coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "    geodesic_features_data = skin_geodesic_distances_df\n",
    "    geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "    for i in range(geodesic_features_data.shape[1]):\n",
    "        if geodesic_features_data.iloc[0, i]:\n",
    "            geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "        else:\n",
    "            geodesic_X_array[i, 0] = 0\n",
    "\n",
    "        if geodesic_features_data.iloc[1, i]:\n",
    "            geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "        else:\n",
    "            geodesic_X_array[i, 1] = 0\n",
    "\n",
    "        if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "            geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "        else:\n",
    "            geodesic_X_array[i, 2] = 0\n",
    "\n",
    "    geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "    X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "    X_test = []\n",
    "\n",
    "    X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "    X_train = X_imputer.fit_transform(X_train)\n",
    "    #X_test = X_imputer.transform(X_test)\n",
    "\n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    #X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "    #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "    #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "    if train_now:\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        trained_model = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "        #validation_losses.append(validation_loss)\n",
    "        #test_losses.append(test_loss)\n",
    "    else:\n",
    "        model_to_load_folder = \"IXI/\"\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "    if save_model and train_now:\n",
    "        timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "        torch.save(trained_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "        pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "        documentation = [\n",
    "            f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "            f\"number_of_training_samples: {train_indices.size}\",\n",
    "            f\"model: {mlp}\",\n",
    "        ]\n",
    "        with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9d737-4073-41b0-b570-93c4b35f45c3",
   "metadata": {},
   "source": [
    "# ADNI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb2ce3-15eb-4ef7-b27e-04b66c7b2569",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d57f0b-443c-4c2b-b62b-c22667f2db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_scale_factor = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afd622-fbac-471d-a755-d09ebc932d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = media_folder+\"/MRI_datasets/\"\n",
    "current_dataset_name = 'ADNI'\n",
    "dataset_filename = 'Dataset_Chamfer.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd0d4b-40dc-40f3-9810-4244a0f1c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_subject_dataframe = pd.ExcelFile(datasets_folder+current_dataset_name+'/'+dataset_filename)\n",
    "current_sheet_names = current_subject_dataframe.sheet_names\n",
    "current_num_of_sheets = len(current_sheet_names)\n",
    "\n",
    "skin_coordinates_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Skin coordinates')\n",
    "skin_normals_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Skin normals')\n",
    "skin_geodesic_distances_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Skin distances')\n",
    "inverse_matrices_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Inverse transformations')\n",
    "stats_index = next(i for i in range(len(current_sheet_names)) if current_sheet_names[i]=='Stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81771a3c-bfc6-430d-a511-683c34415238",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_coordinates_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=skin_coordinates_index, index_col=0)\n",
    "skin_normals_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=skin_normals_index, index_col=0)\n",
    "skin_geodesic_distances_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=skin_geodesic_distances_index, index_col=0)\n",
    "inverse_transformations_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=inverse_matrices_index, index_col=0)\n",
    "stats_df = pd.read_excel(datasets_folder+current_dataset_name+'/'+dataset_filename, sheet_name=stats_index, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393362d-7626-4932-97bc-e0ca9b4add6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_coordinates_columns_names = list(skin_coordinates_df.columns)\n",
    "only_coordinates_columns_indices = []\n",
    "\n",
    "for i in range(len(skin_coordinates_columns_names)):\n",
    "    if 'indices' not in skin_coordinates_columns_names[i]:\n",
    "        only_coordinates_columns_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e3c8b-24e2-443f-8225-cba2bd9b62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    max_euclidean_distance = 75e-3 # that's a lot\n",
    "\n",
    "    relevant_indices = []\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(skin_coordinates_df.index[:num_of_light_landmarks]):\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = valid_coordinates_rows#np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "\n",
    "        desired_landmark_coordinates_mean = np.mean(desired_landmark_subjects_coordinates[valid_rows, :], axis=0)\n",
    "        euclidean_distances = np.linalg.norm(desired_landmark_subjects_coordinates[valid_rows, :]-desired_landmark_coordinates_mean, axis=1)\n",
    "        desired_landmark_relevant_indices = np.where(euclidean_distances<max_euclidean_distance)[0]\n",
    "        relevant_indices.append(desired_landmark_relevant_indices)\n",
    "\n",
    "    only_valid_score_subjects_rows = relevant_indices[0]\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(skin_coordinates_df.index[:num_of_light_landmarks]):\n",
    "        only_valid_score_subjects_rows = np.intersect1d(relevant_indices[desired_landmark_index], only_valid_score_subjects_rows)\n",
    "else:\n",
    "    score_ratio_threshold = 1\n",
    "    only_valid_score_subjects_rows = np.sort(np.argsort(stats_df.loc['unique_correspondence_final_loss', :].values)[:int(score_ratio_threshold*stats_df.shape[1])])\n",
    "    #score_threshold = 2\n",
    "    #only_valid_score_subjects_rows = np.where(stats_df.loc['unique_correspondence_final_loss', :].values<score_threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b56398-2549-49bf-ba59-262628a3b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    array_folder = datasets_folder+current_dataset_name+'/'\n",
    "    array_filename = 'chamfer_distance_subjects_names' # only_valid_score_subjects_names / chamfer_distance_subjects_names\n",
    "    array_filetype = '.npy'\n",
    "\n",
    "    array_path = array_folder + array_filename + array_filetype\n",
    "    \n",
    "    if 0:\n",
    "        with open(array_path, 'wb') as file:\n",
    "            np.save(file, only_valid_score_subject_names)\n",
    "    else:\n",
    "        with open(array_path, 'rb') as file:\n",
    "            only_valid_score_subject_names = np.load(file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36307824-97ac-448f-998d-528db4346c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subject_names = np.array(skin_coordinates_df.columns[0::4])\n",
    "only_valid_score_subjects_rows = []\n",
    "for current_name in only_valid_score_subject_names:\n",
    "    only_valid_score_subjects_rows.append(np.where(all_subject_names==f'{current_name}_indices')[0][0])\n",
    "    \n",
    "only_valid_score_subjects_rows = np.array(only_valid_score_subjects_rows)\n",
    "only_valid_score_subject_names = [current_subject_name[:-8] for current_subject_name in all_subject_names[only_valid_score_subjects_rows]]\n",
    "only_valid_score_subject_names = np.array(only_valid_score_subject_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c0299-6aab-437d-8d2c-26c50520512c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad99f4e-be69-4916-bdae-9db195bda157",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "ablation_study = False\n",
    "MLP_nn = MLP_3 # MLP_2 / MLP_3 / MLP_4\n",
    "if MLP_nn==MLP_2:\n",
    "    MLP_folder = 'MLP_2/'\n",
    "elif MLP_nn==MLP_3:\n",
    "    MLP_folder = 'MLP_3/'\n",
    "else: #MLP_nn==MLP_4\n",
    "    MLP_folder = 'MLP_4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731dd59-1151-403a-a002-3f4dfc964a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ablation_study==False:\n",
    "    X_perturbations = np.zeros((only_valid_score_subjects_rows.size, 3*len(MRI_facial_landmarks)))\n",
    "else:\n",
    "    added_facial_landmarks_noise_norm = 1 #mm\n",
    "    \n",
    "    # https://stackoverflow.com/questions/6283080/random-unit-vector-in-multi-dimensional-space\n",
    "    unnormalized_perturbations = np.random.normal(loc=0, scale=1, size=(int(only_valid_score_subjects_rows.size*len(MRI_facial_landmarks)), 3))\n",
    "    unnormalized_perturbations_magnitudes = np.linalg.norm(unnormalized_perturbations, axis=1)\n",
    "    normalized_perturbations = unnormalized_perturbations/unnormalized_perturbations_magnitudes.reshape(-1, 1)\n",
    "\n",
    "    perturbation_magnitudes = np.random.normal(loc=added_facial_landmarks_noise_norm, scale=0.5, size=int(only_valid_score_subjects_rows.size*len(MRI_facial_landmarks))).reshape(-1, 1)    \n",
    "    \n",
    "    perturbations = normalized_perturbations*1*registration_scale_factor*perturbation_magnitudes\n",
    "    \n",
    "    X_perturbations = perturbations.reshape(-1, len(MRI_facial_landmarks)*3)\n",
    "\n",
    "X_perturbations = np.concatenate((X_perturbations, np.zeros((only_valid_score_subjects_rows.size, 3))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac473626-3b8e-4459-ad2e-927714b1739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "kfold_train_indices = []\n",
    "kfold_test_indices = []\n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(kfold.split(np.arange(only_valid_score_subjects_rows.size))):\n",
    "    kfold_train_indices.append(train_indices)\n",
    "    kfold_test_indices.append(test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4231b-021d-4835-a625-2ecd3a1270f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Euclidean coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d622419-9906-41ea-b9d6-8bbbb0af7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder_path = media_folder+\"/3DMM/Trained_models/pytorch_MLP/Coordinates/\"+MLP_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc5c50-1131-426e-8f0c-6ee07cd288a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83787651-f07f-43ae-aa87-2c1f938727b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features_set_name = 'MRI_facial_landmarks'\n",
    "#chosen_features_set_index = [idx for idx, key in enumerate(list(feature_sets.items()) ) if key[0] == chosen_features_set_name][0]\n",
    "desired_decimation_percentage = str(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a334415-3ae1-4ac9-9c1d-a62c9c502868",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_MAE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_MAE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "ADNI_Euclidean_MSE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_MSE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "ADNI_Euclidean_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_std_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "ADNI_Euclidean_perpendicular_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_tangent_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_distances_ratios = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_distances_ratios_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0fdae-6a15-419a-801f-b5d9532879b4",
   "metadata": {},
   "source": [
    "#### Synthetic model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62207190-b15a-456b-8ed2-eeb3a6a2f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"3DMM/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"3DMM/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = joblib.load(trained_folder_path+\"3DMM/\"+scaler_filename) \n",
    "        #X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_test.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"3DMM/\"+model_filename)))\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "        #validation_losses.append(validation_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        \n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27c0be-4198-4760-a0ca-c58fe0a71710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83158f-7f50-468c-9c41-982c0d51b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3baa0-6a8e-470b-8a8c-886b6f841116",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_MAE[0, :] = mean_errors\n",
    "ADNI_Euclidean_MAE_mean[0, :] = mean_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68d870-836b-46a6-b748-0116720081e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_MSE[0, :] = mean_squared_errors\n",
    "ADNI_Euclidean_MSE_mean[0, :] = mean_squared_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160a060-199d-4211-9c86-68670b50d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_std[0, :] = std_errors\n",
    "ADNI_Euclidean_std_mean[0, :] = std_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fc3a7-26b6-4e86-836d-a2bd65733be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_perpendicular_distances[0, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_tangent_distances[0, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_distances_ratios[0, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_distances_ratios_std[0, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18e5cf-9c64-4de0-af34-c02ec9496595",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_MSE[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_distances_ratios[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddaf92d-d44f-48ca-b477-df5ec26995d4",
   "metadata": {},
   "source": [
    "#### IXI model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95f583-2ca9-4011-a9e4-4446a1d54f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"IXI_ALL/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"IXI_ALL/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = joblib.load(trained_folder_path+\"IXI_ALL/\"+scaler_filename) \n",
    "        #X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "        \n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"IXI_ALL/\"+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a9cf4-6c4f-48ae-b781-4a930eee6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048577d0-5cc5-48cf-b4cf-a048efc03474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6c2a8-ed89-4818-a19c-918e59057134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_MSE[1, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce809e-a013-4532-b024-30ca26e08eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_std[1, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd552c9-6fe2-47a4-b257-387da83d39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_perpendicular_distances[1, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_tangent_distances[1, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_distances_ratios[1, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_distances_ratios_std[1, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194ac06-bbbe-44eb-96a1-1e78a4fcf092",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'other_dataset'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_MSE[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_distances_ratios[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8d4c5-92c2-4919-a572-37b4310819a6",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f6bf8-5ce1-45c5-8f2d-3fe139d556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_ADNI/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59143f-be6c-4c07-9b0b-9b29b030fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        if train_now:\n",
    "            model_to_load_folder = \"3DMM/\"\n",
    "        else:\n",
    "            model_to_load_folder = \"3DMM_ADNI/\"\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        if train_now:\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                #f\"landmark_names_being_used: {features_to_use}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22608a-93c6-491c-a13a-e9e21799f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfa02d-f749-4dfe-8b9a-80554783df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dabbeb-e482-4ce3-8376-44265fc8cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ac882-b4d9-4feb-a88a-1ab87b617904",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_MSE[2, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4938f-e5df-469b-836f-2ffcdba4ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cf47e-8381-4bbc-915f-00bd424c5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_std[2, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd20b3-4f88-4015-8e78-95e461df76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_perpendicular_distances[2, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_tangent_distances[2, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_distances_ratios[2, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_distances_ratios_std[2, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6904a-dda4-42c7-94c7-3a862aea2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_MSE[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_distances_ratios[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c1fd0-c8db-4798-915c-17e5736eec76",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9a026-ff97-4cf1-ba09-2faa165f93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_ADNI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221de76-9aa8-4c3f-9da7-bed151b4a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "    if train_now:\n",
    "        model_to_load_folder = \"3DMM/\"\n",
    "    else:\n",
    "        model_to_load_folder = \"3DMM_ADNI_ALL/\"\n",
    "\n",
    "    most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "    model_filename = most_recent_trained_model_filenames[0]\n",
    "    scaler_filename = most_recent_trained_model_filenames[1]\n",
    "    documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "    with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "        documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "    desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "    desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "    desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "    if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "        valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "    else:\n",
    "        valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "    valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "    #__________________________________________________________________________________________________________\n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "    \n",
    "    #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "    features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "    features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "    features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "    X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "    for i in range(X_array.shape[0]):\n",
    "        X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "    X_array = X_array[valid_rows, :]\n",
    "\n",
    "    X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "    X_test = []\n",
    "    \n",
    "    X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "    X_train = X_imputer.fit_transform(X_train)\n",
    "    #X_test = X_imputer.transform(X_test)\n",
    "    \n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    #X_test = X_standard_scaler.transform(X_test)\n",
    "    \n",
    "    #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "    #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "    \n",
    "    mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "    mlp.load_state_dict(change_keys(torch.load(rained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "    if 1:\n",
    "        for i, param in enumerate(mlp.parameters()):\n",
    "            param.requires_grad=True\n",
    "\n",
    "    if train_now:\n",
    "        predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "        validation_losses.append(validation_loss)\n",
    "        #test_losses.append(test_loss)\n",
    "    else:\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "\n",
    "    if save_model and train_now:\n",
    "        timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "        torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "        pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "        documentation = [\n",
    "            f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "            #f\"landmark_names_being_used: {features_to_use}\",\n",
    "            f\"number_of_training_samples: {train_indices.size}\",\n",
    "            f\"model: {mlp}\",\n",
    "        ]\n",
    "        with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ac3fe-0093-4591-b9d4-d8341ade967e",
   "metadata": {},
   "source": [
    "#### Learn from scratch - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f2b95-2948-47bd-8734-b2c059736e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"ADNI/\"\n",
    "train_now = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f1ce0-79c3-4a75-9496-17a7f14a9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"ADNI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                #f\"landmark_names_being_used: {features_to_use}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97698d-92df-4208-b966-be891a869076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ablation Study')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "        features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "        for i in range(X_array.shape[0]):\n",
    "            X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        X_array = X_array[valid_rows, :]\n",
    "\n",
    "        X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "        X_test = X_array[test_indices, :] + X_perturbations[test_indices, :-3]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"ADNI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                #f\"landmark_names_being_used: {features_to_use}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f8c30-6d9f-47ca-8128-d61d64b2863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3addc6-52b4-4d53-beca-66564d87cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04454617-6b09-48d8-9a13-225616e73a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13564e1-abba-4b3a-9f0d-c178dd905b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9975d6-5279-4ca3-af4f-42bedad4a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6554e5-9bd2-40a0-9780-1bdc7fee3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce9872-04d1-4e08-8fe4-f548676b1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7288f84-57c1-42b0-aaea-ae79d95d57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_MSE[3, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b2ee3-1b21-4d7c-9887-62b5e5f10ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_std[3, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b644ac-7c2f-4343-9437-337b421a69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_perpendicular_distances[3, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_tangent_distances[3, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_distances_ratios[3, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_distances_ratios_std[3, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4504e63-26d2-43db-85c6-006ec5e1f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_MSE[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_distances_ratios[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fad47b-ca64-4634-af27-9c6ac6edef7b",
   "metadata": {},
   "source": [
    "#### Learn from scratch - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6da567-88ef-4847-852e-15c51ea6ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"ADNI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45f7f9-2623-46de-ba2c-59afe5fb4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "    desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "    desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "    desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "    if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "        valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "    else:\n",
    "        valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "    valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "    #__________________________________________________________________________________________________________\n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "\n",
    "    #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "    features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "    features_data = skin_coordinates_df.loc[features_to_use, :]\n",
    "    features_data_subjects_coordinates = features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "    X_array = np.zeros((int(features_data_subjects_coordinates.shape[0]/output_size), output_size*len(features_to_use)))\n",
    "\n",
    "    for i in range(X_array.shape[0]):\n",
    "        X_array[i, :] = np.array(features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "    X_array = X_array[valid_rows, :]\n",
    "\n",
    "    X_train = X_array[train_indices, :] + X_perturbations[train_indices, :-3]\n",
    "    X_test = []\n",
    "\n",
    "    X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "    X_train = X_imputer.fit_transform(X_train)\n",
    "    #X_test = X_imputer.transform(X_test)\n",
    "\n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    #X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "    #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "    #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "    if train_now:\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "                \n",
    "        trained_model = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "        #test_losses.append(test_loss)\n",
    "\n",
    "    if save_model and train_now:\n",
    "        timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "        torch.save(trained_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "        pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "        documentation = [\n",
    "            f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "            #f\"landmark_names_being_used: {features_to_use}\",\n",
    "            f\"number_of_training_samples: {train_indices.size}\",\n",
    "            f\"model: {mlp}\",\n",
    "        ]\n",
    "        with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c890b-0073-4ecf-b55c-f707a4d6ba08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Euclidean coordinates and Geodesic distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5229220-3a16-4890-81e3-b588a7104352",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder_path = media_folder+\"/3DMM/Trained_models/pytorch_MLP/Coordinates_Geodesic/\"+MLP_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a7947-14ee-469f-a958-bfba96acef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d3fc0-2d73-4907-aa7a-1fa071eefc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features_set_name = 'MRI_facial_landmarks'\n",
    "#chosen_features_set_index = [idx for idx, key in enumerate(list(feature_sets.items()) ) if key[0] == chosen_features_set_name][0]\n",
    "desired_decimation_percentage = str(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b8bd1-e29c-401e-a341-92f4fb822e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_MAE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_Geodesic_MAE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "ADNI_Euclidean_Geodesic_MSE = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_Geodesic_MSE_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "ADNI_Euclidean_Geodesic_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_Geodesic_std_mean = np.zeros((1, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "\n",
    "ADNI_Euclidean_Geodesic_perpendicular_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_Geodesic_tangent_distances = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_Geodesic_distances_ratios = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "ADNI_Euclidean_Geodesic_distances_ratios_std = np.zeros((4, len(selected_EEG_10_20_landmark_names), n_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc066ae-e98e-4271-b9f6-ee6ace01b7fc",
   "metadata": {},
   "source": [
    "#### Synthetic model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec6bec-c556-4832-b387-804b356cca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"3DMM/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"3DMM/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"3DMM/\"+model_filename)))\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "        #validation_losses.append(validation_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67516283-d0d4-457e-94a8-46cd46c7f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311e13b-4b5d-4cf9-99b1-a621b2ee6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a5c6f-5ead-4d52-a3d0-68fec50bef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_MAE[0, :] = mean_errors\n",
    "ADNI_Euclidean_Geodesic_MAE_mean[0, :] = mean_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cab7e6-0908-4dd6-869a-1659a23be934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_MSE[0, :] = mean_squared_errors\n",
    "ADNI_Euclidean_Geodesic_MSE_mean[0, :] = mean_squared_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbed9f-35a1-481e-a663-664bb0094c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_std[0, :] = std_errors\n",
    "ADNI_Euclidean_Geodesic_std_mean[0, :] = std_errors_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82aaf3f-bdae-4e90-a748-3a6c61a312f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_perpendicular_distances[0, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_Geodesic_tangent_distances[0, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios[0, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios_std[0, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea17c1-f760-45a8-871f-56b73a7fd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_MSE[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_distances_ratios[0, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e1d7e-d029-4eaa-802f-b3f57d192ffb",
   "metadata": {},
   "source": [
    "#### IXI model - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062111f-9db1-41e7-8caa-7b17f1aaae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+\"IXI_ALL/\", desired_landmark_name)\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+\"IXI_ALL/\"+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        output_size = desired_landmark_subjects_coordinates[valid_rows, :].shape[1]\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = joblib.load(trained_folder_path+\"IXI_ALL/\"+scaler_filename) \n",
    "        #X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+\"IXI_ALL/\"+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46917c08-4b35-4f0b-84c1-4bd167d155e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f16c6-c899-44dc-9617-18342a215ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e021fdaa-9b90-45c0-9c23-e8397fe73ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_MSE[1, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a3d15-6001-486b-9ded-28e780485be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_std[1, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32388dd8-07f1-4734-a814-9d929d5b4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_perpendicular_distances[1, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_Geodesic_tangent_distances[1, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios[1, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios_std[1, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119865f-5e39-4c70-b7d6-37fe4fc28355",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'other_dataset_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_MSE[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_distances_ratios[1, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736470f0-599d-481d-b40e-caf3be631d36",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f580fd-2c2d-4d3b-b07d-8d190597f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_ADNI/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d4e70-1790-410c-aa1c-2d1a9fa54a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        if train_now:\n",
    "            model_to_load_folder = \"3DMM/\"\n",
    "        else:\n",
    "            model_to_load_folder = \"3DMM_ADNI/\"\n",
    "\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        if train_now:\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "            #validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df6a82-5447-48a0-bca4-f8097e425f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1547a0-5dc2-4cc3-a54c-d82da9ae0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c884a-4f67-4d39-9da4-762344b3a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625682e-7eb6-4858-93fd-d79ee0b8ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_MSE[2, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcec2b8-2aac-4c7b-889f-9968d550b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_std[2, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20548b2c-cb71-4ec4-a1be-c7f6cb605128",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_perpendicular_distances[2, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_Geodesic_tangent_distances[2, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios[2, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios_std[2, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa678e-15b3-4f35-bcc8-3df6b64562cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'synthetic_fine_tuned_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_MSE[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_distances_ratios[2, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801b497-de89-49f3-85d3-b3207f801c0d",
   "metadata": {},
   "source": [
    "#### Fine-tune Synthetic model - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4d986-b1cf-490b-bfaa-cd75043a9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"3DMM_ADNI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8cd3a-2f01-422c-aff6-824d103003e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "\tif train_now:\n",
    "\t\tmodel_to_load_folder = \"3DMM/\"\n",
    "\telse:\n",
    "\t\tmodel_to_load_folder = \"3DMM_ADNI_ALL/\"\n",
    "\n",
    "\tmost_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "\tmodel_filename = most_recent_trained_model_filenames[0]\n",
    "\tscaler_filename = most_recent_trained_model_filenames[1]\n",
    "\tdocumentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "\twith open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "\t\tdocumentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "\tdesired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "\tdesired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "\tdesired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "\tif np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "\t\tvalid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "\telse:\n",
    "\t\tvalid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "\tvalid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "\t#__________________________________________________________________________________________________________   \n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "\n",
    "\tcoordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "\tcoordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "\tcoordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "\tcoordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "\tfor i in range(coordinates_X_array.shape[0]):\n",
    "\t\tcoordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "\tcoordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "\tgeodesic_features_data = skin_geodesic_distances_df\n",
    "\tgeodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "\tfor i in range(geodesic_features_data.shape[1]):\n",
    "\t\tif geodesic_features_data.iloc[0, i]:\n",
    "\t\t\tgeodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "\t\telse:\n",
    "\t\t\tgeodesic_X_array[i, 0] = 0\n",
    "\n",
    "\t\tif geodesic_features_data.iloc[1, i]:\n",
    "\t\t\tgeodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "\t\telse:\n",
    "\t\t\tgeodesic_X_array[i, 1] = 0\n",
    "\n",
    "\t\tif np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "\t\t\tgeodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "\t\telse:\n",
    "\t\t\tgeodesic_X_array[i, 2] = 0\n",
    "\n",
    "\tgeodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "\tX_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "\tX_test = []\n",
    "\n",
    "\tX_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "\tX_train = X_imputer.fit_transform(X_train)\n",
    "\t#X_test = X_imputer.transform(X_test)\n",
    "\n",
    "\tX_standard_scaler = StandardScaler()\n",
    "\tX_train = X_standard_scaler.fit_transform(X_train)\n",
    "\t#X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "\t#print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "\t#print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "\tmlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\tmlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "\tif 1:\n",
    "\t\tfor i, param in enumerate(mlp.parameters()):\n",
    "\t\t\tparam.requires_grad=True\n",
    "\n",
    "\tif train_now:\n",
    "\t\tpredictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='fine_tune', n_jobs_num=1)\n",
    "\t\tvalidation_losses.append(validation_loss)\n",
    "\t\t#test_losses.append(test_loss)\n",
    "\telse:\n",
    "\t\tpredictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "\tif save_model and train_now:\n",
    "\t\ttimestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\t\ttimestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "\t\ttorch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "\t\tpickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "\t\tdocumentation = [\n",
    "\t\t\tf\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "\t\t\tf\"number_of_training_samples: {train_indices.size}\",\n",
    "\t\t\tf\"model: {mlp}\",\n",
    "\t\t]\n",
    "\t\twith open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "\t\t\ttxt_file.write(\"\\n\".join(documentation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954875ba-e51c-4082-9def-51cf7b3dac26",
   "metadata": {},
   "source": [
    "#### Learn from scratch - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb57fc-8936-4338-a471-ee5071dda3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"ADNI/\"\n",
    "train_now = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3be5b7-b0aa-406a-b034-c5ea3323da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        #for i in range (len(coordinates_features_to_use)):\n",
    "        #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"ADNI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acfd0b-c3e9-4984-960d-f71c566e8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ablation Study')\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for fold_index in range(n_splits):\n",
    "    train_indices = kfold_train_indices[fold_index]\n",
    "    test_indices = kfold_test_indices[fold_index]\n",
    "    print(f\"Started fold {fold_index+1}/{n_splits}\")\n",
    "    \n",
    "    for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "        print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "        desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "        desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "        desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "        if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "            valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "        else:\n",
    "            valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "        valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "        #__________________________________________________________________________________________________________\n",
    "        y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "        y_train = y_array[train_indices, :]\n",
    "        y_test = y_array[test_indices, :]\n",
    "        output_size = y_test.shape[1]\n",
    "\n",
    "        #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "        #for i in range (len(coordinates_features_to_use)):\n",
    "        #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "        coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "        coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "        coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "        coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "        for i in range(coordinates_X_array.shape[0]):\n",
    "            coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "        coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "        geodesic_features_data = skin_geodesic_distances_df\n",
    "        geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "        for i in range(geodesic_features_data.shape[1]):\n",
    "            if geodesic_features_data.iloc[0, i]:\n",
    "                geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 0] = 0\n",
    "\n",
    "            if geodesic_features_data.iloc[1, i]:\n",
    "                geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "            else:\n",
    "                geodesic_X_array[i, 1] = 0\n",
    "\n",
    "            if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "                geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "            else:\n",
    "                geodesic_X_array[i, 2] = 0\n",
    "\n",
    "        geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "        X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "        X_test = np.concatenate((coordinates_X_array[test_indices, :], geodesic_X_array[test_indices, :]), axis=1) + X_perturbations[test_indices, :]\n",
    "\n",
    "        X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "        X_train = X_imputer.fit_transform(X_train)\n",
    "        X_test = X_imputer.transform(X_test)\n",
    "\n",
    "        X_standard_scaler = StandardScaler()\n",
    "        X_train = X_standard_scaler.fit_transform(X_train)\n",
    "        X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "        #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "        #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "        if train_now:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "            if 1:\n",
    "                for i, param in enumerate(mlp.parameters()):\n",
    "                    param.requires_grad=True\n",
    "\n",
    "            predictions, validation_loss, test_loss, lowest_validation_loss_model, lowest_validation_loss_epoch = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1)\n",
    "            validation_losses.append(validation_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        else:\n",
    "            model_to_load_folder = \"ADNI/\"\n",
    "            most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "            model_filename = most_recent_trained_model_filenames[0]\n",
    "            scaler_filename = most_recent_trained_model_filenames[1]\n",
    "            documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "            with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "                documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "            predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "        if save_model and train_now:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(lowest_validation_loss_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_model')\n",
    "            pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"model: {mlp}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_'+str(fold_index)+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        predictions_original_space = np.zeros((predictions.shape))\n",
    "        for i in range(predictions_original_space.shape[0]):\n",
    "            current_prediction = predictions[i, :].to(\"cpu\")\n",
    "            predictions_original_space[i, :] = transform_to_original_space(current_prediction, test_indices[i]).squeeze()\n",
    "\n",
    "        y_train_original_space = np.zeros((y_train.shape))\n",
    "        for i in range(y_train_original_space.shape[0]):\n",
    "            current_y = y_train[i, :]\n",
    "            y_train_original_space[i, :] = transform_to_original_space(current_y, train_indices[i]).squeeze()\n",
    "\n",
    "        y_test_original_space = np.zeros((y_test.shape))\n",
    "        for i in range(y_test_original_space.shape[0]):\n",
    "            current_y = y_test[i, :]\n",
    "            y_test_original_space[i, :] = transform_to_original_space(current_y, test_indices[i]).squeeze()\n",
    "\n",
    "        prediction_errors = np.linalg.norm(predictions_original_space-y_test_original_space, axis=1)\n",
    "        mean_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors)\n",
    "        mean_squared_errors[desired_landmark_index, fold_index] = np.mean(prediction_errors**2)\n",
    "        print(f\"RMSE: {1000*np.sqrt(mean_squared_errors[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors[desired_landmark_index, fold_index] = np.std(prediction_errors)\n",
    "\n",
    "        prediction_errors_mean = np.linalg.norm(np.mean(y_train_original_space, axis=0)-y_test_original_space, axis=1)\n",
    "        mean_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean)\n",
    "        mean_squared_errors_mean[desired_landmark_index, fold_index] = np.mean(prediction_errors_mean**2)\n",
    "        print(f\"Mean guess RMSE: {1000*np.sqrt(mean_squared_errors_mean[desired_landmark_index, fold_index])}\")\n",
    "\n",
    "        std_errors_mean[desired_landmark_index, fold_index] = np.std(prediction_errors_mean)\n",
    "\n",
    "        perpendicular_distances = []\n",
    "        tangent_distances = []\n",
    "\n",
    "        for i, current_subject in enumerate(test_indices):\n",
    "            current_plane_point = y_test_original_space[i, :]\n",
    "            current_other_point = predictions_original_space[i, :]\n",
    "\n",
    "            current_landmark_index = list(skin_normals_df.index).index(desired_landmark_name)\n",
    "            current_plane_normal = skin_normals_df.iloc[current_landmark_index, 3*i:3*(i+1)].values\n",
    "\n",
    "            if np.linalg.norm(current_plane_normal)>0:\n",
    "                current_perpendicular_distance, current_tangent_distance = project_distances(current_plane_normal, current_plane_point, current_other_point)\n",
    "                perpendicular_distances.append(current_perpendicular_distance)\n",
    "                tangent_distances.append(current_tangent_distance)\n",
    "\n",
    "        perpendicular_distances = np.array(perpendicular_distances)\n",
    "        tangent_distances = np.array(tangent_distances)\n",
    "\n",
    "        perpendicular_distances_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_distances)\n",
    "        tangent_distances_mean[desired_landmark_index, fold_index] = np.mean(tangent_distances)\n",
    "        perpendicular_ratios = np.abs(perpendicular_distances/tangent_distances)\n",
    "        distances_ratios_mean[desired_landmark_index, fold_index] = np.mean(perpendicular_ratios)\n",
    "        distances_ratios_std[desired_landmark_index, fold_index] = np.std(perpendicular_ratios)\n",
    "        \n",
    "    print(f\"Fold index {fold_index+1} mean RMSE across landmarks: {np.mean(1000*np.sqrt(mean_squared_errors[:, fold_index]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa537e-5508-4d59-ad1e-584531cda603",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_mean_test_losses = []\n",
    "for i in range(len(selected_EEG_10_20_landmark_names)):\n",
    "    i_landmark_test_losses = []\n",
    "    for j in range(int(len(test_losses)/len(selected_EEG_10_20_landmark_names))):\n",
    "        ij_losses = np.array(test_losses[i+j*len(selected_EEG_10_20_landmark_names)]).reshape(1, -1)\n",
    "        #print('ij_losses', ij_losses.shape)\n",
    "        i_landmark_test_losses.append(ij_losses)\n",
    "    i_landmark_test_losses = np.vstack(i_landmark_test_losses)\n",
    "    #print('i_landmark_test_losses', i_landmark_test_losses.shape)\n",
    "    mean_values = np.mean(i_landmark_test_losses, axis=0)\n",
    "    #print('mean_values', mean_values.shape)\n",
    "    landmarks_mean_test_losses.append(mean_values)\n",
    "landmarks_mean_test_losses = np.vstack(landmarks_mean_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5279f2a-37a5-4b9c-9600-348692bf8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "        y=np.mean(landmarks_mean_test_losses, axis=0),\n",
    "        mode='lines',\n",
    "        name='Mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for i, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(landmarks_mean_test_losses.shape[1]),\n",
    "            y=landmarks_mean_test_losses[i, :],\n",
    "            mode='lines',\n",
    "            name=desired_landmark_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8765b14-f802-4b20-be91-0ffe445f6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Test_losses_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = landmarks_mean_test_losses\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac6b10-ca41-45d4-91a0-7d0a35ee25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505fd23-d9b8-41b9-9214-285b149dcb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c5bc1-9b24-404f-b57c-0af64cb3c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE: {np.mean(1000*np.sqrt(mean_squared_errors))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa42d4d-e82d-43ae-8648-5e8c5f9ff17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ablation Study Mean RMSE over folds: {np.mean(1000*np.sqrt(mean_squared_errors), axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9460ac-c43e-426d-aa93-98c16a22d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_MSE[3, :] = mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c5696-addc-488d-ab83-73414f4996e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_std[3, :] = std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0e63a-eb8a-4612-a105-ddeb84e44adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNI_Euclidean_Geodesic_perpendicular_distances[3, :] = perpendicular_distances_mean\n",
    "ADNI_Euclidean_Geodesic_tangent_distances[3, :] = tangent_distances_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios[3, :] = distances_ratios_mean\n",
    "ADNI_Euclidean_Geodesic_distances_ratios_std[3, :] = distances_ratios_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e642ef-7ffe-49d7-beb2-a7ce245c1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "experiment_type = 'same_dataset_CG'\n",
    "\n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Prediction_arrays/\"+MLP_folder\n",
    "if not ablation_study:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_{timestamp_string}\"\n",
    "else:\n",
    "    array_filename = f\"{current_dataset_name}_{trained_folder_path.split('/')[-2]}_{experiment_type}_ablation_study_{timestamp_string}\"\n",
    "array_filetype = '.npy'\n",
    "\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_MSE[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)\n",
    "        \n",
    "array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/Projected_prediction_ratio_arrays/\"\n",
    "array_path = array_folder + array_filename + array_filetype\n",
    "\n",
    "array_to_save = ADNI_Euclidean_Geodesic_distances_ratios[3, :]\n",
    "\n",
    "if 1:\n",
    "    with open(array_path, 'wb') as file:\n",
    "        np.save(file, array_to_save)\n",
    "else:\n",
    "    with open(array_path, 'rb') as file:\n",
    "        array_to_save = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03733a-fd7e-4a63-a8f2-8cd5c8548a8a",
   "metadata": {},
   "source": [
    "#### Learn from scratch - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b6256-f97d-43b7-81fe-3bc223f1acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_path = trained_folder_path+\"ADNI_ALL/\"\n",
    "train_now = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9d27a-0778-4ff5-8928-c192fac7a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "mean_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "mean_squared_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "std_errors_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "perpendicular_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "tangent_distances_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_mean = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "distances_ratios_std = np.zeros((len(selected_EEG_10_20_landmark_names), n_splits))\n",
    "validation_losses = []\n",
    "#test_losses = []\n",
    "\n",
    "for desired_landmark_index, desired_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    print(f\"Started {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_landmark_names)}\")\n",
    "\n",
    "    desired_landmark_data = skin_coordinates_df.loc[desired_landmark_name, :]\n",
    "    desired_landmark_subjects_coordinates = desired_landmark_data.iloc[np.array(only_coordinates_columns_indices)]\n",
    "    desired_landmark_subjects_coordinates = np.array(desired_landmark_subjects_coordinates).reshape(-1, 3)\n",
    "    if np.where(np.isnan(desired_landmark_subjects_coordinates)==True)[0].size>0:\n",
    "        valid_coordinates_rows = np.unique(np.where(np.isnan(desired_landmark_subjects_coordinates)==False)[0])\n",
    "    else:\n",
    "        valid_coordinates_rows = np.arange(desired_landmark_subjects_coordinates.shape[0])\n",
    "\n",
    "    valid_rows = np.intersect1d(valid_coordinates_rows, only_valid_score_subjects_rows)\n",
    "    #__________________________________________________________________________________________________________\n",
    "    y_array = desired_landmark_subjects_coordinates[valid_rows, :]\n",
    "    train_indices = np.arange(y_array.shape[0])\n",
    "    y_train = y_array[train_indices, :]\n",
    "    y_test = np.array([-1, -1, -1]).reshape(1, 3)\n",
    "    output_size = y_test.shape[1]\n",
    "    #features_to_use = documentation[2].split(': ')[1][1:-1].split(', ')\n",
    "    #for i in range (len(coordinates_features_to_use)):\n",
    "    #    coordinates_features_to_use[i] = coordinates_features_to_use[i].replace(\"'\", \"\")\n",
    "\n",
    "    coordinates_features_to_use = np.array(MRI_facial_landmarks).astype(str)\n",
    "\n",
    "    coordinates_features_data = skin_coordinates_df.loc[coordinates_features_to_use, :]\n",
    "    coordinates_features_data_subjects_coordinates = coordinates_features_data.iloc[:, np.array(only_coordinates_columns_indices)].T\n",
    "\n",
    "    coordinates_X_array = np.zeros((int(coordinates_features_data_subjects_coordinates.shape[0]/output_size), output_size*len(coordinates_features_to_use)))\n",
    "\n",
    "    for i in range(coordinates_X_array.shape[0]):\n",
    "        coordinates_X_array[i, :] = np.array(coordinates_features_data_subjects_coordinates.iloc[output_size*i:output_size*(i+1), :].T).reshape(-1, 1).T\n",
    "    coordinates_X_array = coordinates_X_array[valid_rows, :]\n",
    "\n",
    "    geodesic_features_data = skin_geodesic_distances_df\n",
    "    geodesic_X_array = np.zeros((geodesic_features_data.shape[1], 3))\n",
    "\n",
    "    for i in range(geodesic_features_data.shape[1]):\n",
    "        if geodesic_features_data.iloc[0, i]:\n",
    "            geodesic_X_array[i, 0] = geodesic_features_data.iloc[0, i]\n",
    "        else:\n",
    "            geodesic_X_array[i, 0] = 0\n",
    "\n",
    "        if geodesic_features_data.iloc[1, i]:\n",
    "            geodesic_X_array[i, 1] = geodesic_features_data.iloc[1, i]\n",
    "        else:\n",
    "            geodesic_X_array[i, 1] = 0\n",
    "\n",
    "        if np.all(np.array(geodesic_features_data.iloc[2:, i])):\n",
    "            geodesic_X_array[i, 2] = np.sum(np.array(geodesic_features_data.iloc[2:, i]))\n",
    "        else:\n",
    "            geodesic_X_array[i, 2] = 0\n",
    "\n",
    "    geodesic_X_array = geodesic_X_array[valid_rows, :]\n",
    "\n",
    "    X_train = np.concatenate((coordinates_X_array[train_indices, :], geodesic_X_array[train_indices, :]), axis=1) + X_perturbations[train_indices, :]\n",
    "    X_test = []\n",
    "\n",
    "    X_imputer = KNNImputer(missing_values=0, n_neighbors=int(X_train.shape[0]/10), weights=\"uniform\")\n",
    "    X_train = X_imputer.fit_transform(X_train)\n",
    "    #X_test = X_imputer.transform(X_test)\n",
    "\n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    #X_test = X_standard_scaler.transform(X_test)\n",
    "\n",
    "    #print(repr(np.std(X_train, axis=0).reshape(-1, 3)))\n",
    "    #print(repr(np.std(X_test, axis=0).reshape(-1, 3)))\n",
    "\n",
    "    if train_now:\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "\n",
    "        if 1:\n",
    "            for i, param in enumerate(mlp.parameters()):\n",
    "                param.requires_grad=True\n",
    "\n",
    "        trained_model = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='train', n_jobs_num=1, num_of_epochs=num_of_epochs)\n",
    "        #validation_losses.append(validation_loss)\n",
    "        #test_losses.append(test_loss)\n",
    "    else:\n",
    "        model_to_load_folder = \"ADNI/\"\n",
    "        most_recent_trained_model_filenames = find_most_recent_trained_files(trained_folder_path+model_to_load_folder, desired_landmark_name, fold_index)\n",
    "\n",
    "        model_filename = most_recent_trained_model_filenames[0]\n",
    "        scaler_filename = most_recent_trained_model_filenames[1]\n",
    "        documentation_filename = most_recent_trained_model_filenames[2]\n",
    "\n",
    "        with open(trained_folder_path+model_to_load_folder+documentation_filename) as documentation_file:\n",
    "            documentation = [line.rstrip() for line in documentation_file]\n",
    "\n",
    "        mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "        mlp.load_state_dict(change_keys(torch.load(trained_folder_path+model_to_load_folder+model_filename)))\n",
    "\n",
    "        predictions, _, test_loss, _, _ = model_choose_and_predict(mlp, X_train, y_train, X_test, y_test, mode='test_only', n_jobs_num=1)\n",
    "\n",
    "    if save_model and train_now:\n",
    "        timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "        torch.save(trained_model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "        pickle.dump(X_standard_scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "        documentation = [\n",
    "            f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "            f\"number_of_training_samples: {train_indices.size}\",\n",
    "            f\"model: {mlp}\",\n",
    "        ]\n",
    "        with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "            txt_file.write(\"\\n\".join(documentation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "0163732830db25dc078d3027c756dfdbec9bc22e784d48b7fd5f8168fb337cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
