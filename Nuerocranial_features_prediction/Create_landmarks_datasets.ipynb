{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e8fcf4-ecc6-470b-bb11-b7cd654d6d60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading and disaplying the 3DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2274a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from datetime import datetime\n",
    "from Prediction_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca76a0d-e07d-4366-bfab-9cf73958a9f2",
   "metadata": {},
   "source": [
    "## Original model including everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb3980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "models_folder = './datasets/TMS/3DMM/UHM_models/'\n",
    "currnet_model = 'UHM' \n",
    "\n",
    "# UHM model with all the components fused together (i.e. ears, inner mouth, and teeth)\n",
    "model_name = 'head_model_global_align'\n",
    "\n",
    "model_file = open(models_folder + model_name + '.pkl', 'rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51923f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning coordinates system to be in millimeter units\n",
    "scale_factor = 100\n",
    "\n",
    "# get model parameteres\n",
    "mean_shape = scale_factor*model_dict['Mean']\n",
    "mean_shape_CCS = mean_shape.reshape(-1,3)\n",
    "eigen_vec = model_dict['Eigenvectors']\n",
    "eigen_vec_num =  model_dict['Eigenvectors'].shape[1]\n",
    "eigen_val = model_dict['EigenValues']\n",
    "trilist = model_dict['Trilist']\n",
    "vertices_num = model_dict['Number_of_vertices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules (landmarks and masks)\n",
    "modules_folder = models_folder + '/Landmarks and masks/'\n",
    "\n",
    "modules_to_load = ['68_land_idxs'] # EEG_10_20_full_model / '49_plus_ears_land_idxs' / '68_land_idxs'\n",
    "\n",
    "landmarks = []\n",
    "landmarks_names = []\n",
    "landmarks_groups = []\n",
    "\n",
    "for currnet_module_name in modules_to_load:\n",
    "    module_file = open(modules_folder + currnet_module_name + '.pkl', 'rb')\n",
    "    currnet_module = pickle.load(module_file)\n",
    "    module_file.close()\n",
    "    if currnet_module_name=='EEG_10_20':\n",
    "        currnet_module_names = list(currnet_module.keys())\n",
    "        currnet_module = np.asarray(list(currnet_module.values()))\n",
    "    else:\n",
    "        currnet_module_names = list(map(str, 1+np.arange(len(currnet_module))))\n",
    "        \n",
    "    landmarks.append(currnet_module)\n",
    "    landmarks_names.append(currnet_module_names)\n",
    "    landmarks_groups.append(np.arange(len(currnet_module)))\n",
    "\n",
    "# turn list of lists into one list\n",
    "landmarks = [item for items in landmarks for item in items]\n",
    "landmarks_names = [item for items in landmarks_names for item in items]\n",
    "\n",
    "num_of_landmarks = len(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b370f-7a02-45fe-9eb6-3254bd2f79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_names_to_remove = ['eyes_mask', 'teeth_mask', 'mouth_teeth_mask']\n",
    "modules_to_remove = []\n",
    "\n",
    "for module_name_to_remove in module_names_to_remove:\n",
    "    module_file_to_remove = open(modules_folder + module_name_to_remove + '.pkl', 'rb')\n",
    "    module_to_remove = pickle.load(module_file_to_remove)\n",
    "    modules_to_remove.append(module_to_remove)\n",
    "    module_file_to_remove.close()\n",
    "    \n",
    "mask_to_remove = np.logical_and(modules_to_remove[0], modules_to_remove[1], modules_to_remove[2])\n",
    "mask_to_remove = ~mask_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62cac9-b344-430e-abca-2c8749406f93",
   "metadata": {},
   "source": [
    "## Lighter model including everything but eyes, teeth and inner mouth cavity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6328036-ddc5-43cf-b6e4-b2796579a647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "light_models_folder = './datasets/TMS/3DMM/UHM_models/'\n",
    "light_currnet_model = 'UHM' \n",
    "\n",
    "# UHM model with all the components fused together (i.e. ears, inner mouth, and teeth)\n",
    "light_model_name = 'head_model_global_align_no_mouth_and_eyes'\n",
    "\n",
    "light_model_file = open(light_models_folder + light_model_name + '.pkl', 'rb')\n",
    "light_model_dict = pickle.load(light_model_file)\n",
    "light_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3054a7-0bf9-48cf-b1a7-99fa02f11efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning cartesian coordinates system to be in millimeter units\n",
    "light_scale_factor = 100\n",
    "\n",
    "# get model parameteres\n",
    "light_mean_shape = light_scale_factor*light_model_dict['Mean']\n",
    "light_mean_shape_CCS = light_mean_shape.reshape(-1,3)\n",
    "light_eigen_vec = light_model_dict['Eigenvectors']\n",
    "light_eigen_vec_num =  light_model_dict['Eigenvectors'].shape[1]\n",
    "light_eigen_val = light_model_dict['EigenValues']\n",
    "light_trilist = light_model_dict['Trilist']\n",
    "light_vertices_num = light_model_dict['Number_of_vertices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b79ce0-437f-49aa-a8f9-63598b9e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules (landmarks and masks)\n",
    "modules_folder = models_folder + '/Landmarks and masks/'\n",
    "\n",
    "modules_to_load = ['EEG_10_20'] # EEG_10_20 / '49_plus_ears_land_idxs' / '68_land_idxs'\n",
    "\n",
    "light_landmarks = []\n",
    "light_landmarks_names = []\n",
    "light_landmarks_groups = []\n",
    "\n",
    "for currnet_module_name in modules_to_load:\n",
    "    module_file = open(modules_folder + currnet_module_name + '.pkl', 'rb')\n",
    "    currnet_module = pickle.load(module_file)\n",
    "    module_file.close()\n",
    "    if currnet_module_name=='EEG_10_20':\n",
    "        currnet_module_names = list(currnet_module.keys())\n",
    "        currnet_module = np.asarray(list(currnet_module.values()))\n",
    "    else:\n",
    "        currnet_module_names = list(map(str, 1+np.arange(len(currnet_module))))\n",
    "        \n",
    "    light_landmarks.append(currnet_module)\n",
    "    light_landmarks_names.append(currnet_module_names)\n",
    "    light_landmarks_groups.append(np.arange(len(currnet_module)))\n",
    "\n",
    "# turn list of lists into one list\n",
    "light_landmarks = [item for items in light_landmarks for item in items]\n",
    "light_landmarks_names = [item for items in light_landmarks_names for item in items]\n",
    "\n",
    "num_of_light_landmarks = len(light_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac03431-c315-4e93-a72b-40ee146634aa",
   "metadata": {},
   "source": [
    "## Matching landmark indices between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34cb191-097a-4d3f-82a2-de50f6ee0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_facial_landmarks = landmarks\n",
    "\n",
    "for current_landmark_index, current_landmark_vertex in enumerate(landmarks):\n",
    "    original_model_coordinates = mean_shape_CCS[current_landmark_vertex]\n",
    "    new_model_vertex_diffs = np.linalg.norm(light_mean_shape_CCS-original_model_coordinates, axis=1)\n",
    "    light_facial_landmarks[current_landmark_index] = np.argmin(new_model_vertex_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b59578-164a-4ac7-ba26-ef25cbb98bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_nasion = 52241\n",
    "light_inion = 36323"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd30814-22c9-4d63-b5b2-0ba046d592d0",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2da8c8-cad3-44a7-8975-d42d11e83a39",
   "metadata": {},
   "source": [
    "## Model Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c88e9a-1507-4654-b028-44c4b4b5e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_light_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7712837-fa78-4cd5-b8f4-0a30ca64502a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if choose_light_model:\n",
    "    landmarks = np.concatenate((light_landmarks, light_facial_landmarks, [light_nasion, light_inion]))\n",
    "    landmarks_names = list(np.concatenate((light_landmarks_names, landmarks_names, ['nasion', 'inion'])))\n",
    "    mean_shape = light_mean_shape\n",
    "    mean_shape_CCS = light_mean_shape_CCS\n",
    "    eigen_vec = light_eigen_vec\n",
    "    eigen_vec_num =  light_eigen_vec_num\n",
    "    eigen_val = light_eigen_val\n",
    "    trilist = light_trilist\n",
    "    vertices_num = light_vertices_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85693d0-fe71-4939-a45d-8906474f1f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfad12-7777-41e0-9a8e-05b8334af447",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_landmarks_names = np.array([37, 40, 43, 46, 49, 55, 31, 9])\n",
    "rigid_facial_landmarks_names = np.array([37, 40, 43, 46, 28, 1, 17])\n",
    "\n",
    "center_of_the_eyebrows = np.array([20, 25])\n",
    "corners_of_the_eyebrows = np.array([18, 22, 23, 27])\n",
    "corners_of_the_eyes = np.array([37, 40, 43, 46])\n",
    "sides_of_the_face = np.array([1, 17])\n",
    "nose_bone = np.array([28, 31])\n",
    "lower_nose = np.array([32, 34, 36])\n",
    "corners_of_the_mouth = np.array([49, 55])\n",
    "chin = np.array([9])\n",
    "\n",
    "facial_landmarks = np.concatenate((center_of_the_eyebrows, corners_of_the_eyebrows, corners_of_the_eyes, sides_of_the_face, nose_bone, lower_nose,\n",
    "                                   corners_of_the_mouth, chin))\n",
    "selected_facial_indices = np.sort(facial_landmarks+num_of_light_landmarks-1)\n",
    "selected_facial_landmark_names = np.take(landmarks_names, selected_facial_indices)\n",
    "\n",
    "selected_EEG_10_20_landmark_names = ['Fz', 'Pz', 'Cz', 'F3', 'F4', 'P3', 'P4', 'O1', 'O2']\n",
    "selected_EEG_10_20_indices = []\n",
    "for current_index, current_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    selected_EEG_10_20_indices.append(light_landmarks_names.index(current_landmark_name))\n",
    "selected_EEG_10_20_indices = np.asarray(selected_EEG_10_20_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bca439-8bcd-4d82-ae92-93a01fe69df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cranial_indices = [7, 13, 89, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6758a0-0161-43f9-9aa7-1b0abb1870f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_landmarks = ['1', '2', '3', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
    "                   '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42',\n",
    "                   '43', '44', '45', '46', '47', '48']\n",
    "input_landmarks = [int(current_landmark) for current_landmark in input_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63efbc-5e48-4388-ad6f-87846a5911e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.concatenate((selected_EEG_10_20_indices, 20+np.array(input_landmarks), cranial_indices))\n",
    "selected_indices_names = np.take(landmarks_names, selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836abd6-86d8-4f87-a4e4-a9f44f7cce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_EEG_10_20_landmark_names = ['Fp1', 'F7', 'T3', 'T5', 'O1', 'O2', 'T6', 'T4', 'F8', 'Fp2']\n",
    "distances_EEG_10_20_indices = []\n",
    "for current_index, current_landmark_name in enumerate(distances_EEG_10_20_landmark_names):\n",
    "    distances_EEG_10_20_indices.append(light_landmarks_names.index(current_landmark_name))\n",
    "distances_EEG_10_20_indices = np.asarray(distances_EEG_10_20_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e54731-8170-4dff-8b0e-d4da8ff3a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_to_find = [landmarks[cranial_indices[:2]], #A1-A2\n",
    "                     landmarks[cranial_indices[2:]], #nasion-inion\n",
    "                     landmarks[distances_EEG_10_20_indices[:2]],\n",
    "                     landmarks[distances_EEG_10_20_indices[1:3]],\n",
    "                     landmarks[distances_EEG_10_20_indices[2:4]],\n",
    "                     landmarks[distances_EEG_10_20_indices[3:5]],\n",
    "                     landmarks[distances_EEG_10_20_indices[4:6]],\n",
    "                     landmarks[distances_EEG_10_20_indices[5:7]],\n",
    "                     landmarks[distances_EEG_10_20_indices[6:8]],\n",
    "                     landmarks[distances_EEG_10_20_indices[7:9]],\n",
    "                     landmarks[distances_EEG_10_20_indices[8:10]],\n",
    "                     landmarks[np.array([distances_EEG_10_20_indices[-1], distances_EEG_10_20_indices[0]])]\n",
    "                    ] \n",
    "distances_to_find_names = ['A1_A2', 'nasion_inion',\n",
    "                           'Fp1_F7', 'F7_T3', 'T3_T5', 'T5_O1', 'O1_O2', 'O2_T6', 'T6_T4', 'T4_F8', 'F8_Fp2', 'Fp2_Fp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209b92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define parameteres\n",
    "create_new = True\n",
    "decimation = True\n",
    "fixed_landmarks = True\n",
    "calculate_rigid = False\n",
    "num_of_instances = int(2.5e4)\n",
    "num_digits_round = 10\n",
    "num_of_landmarks = len(landmarks)\n",
    "decimation_percentages = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23143d5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a961aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new shape by adding deformations to the mean shape using the eigendecomposition\n",
    "def get_new_shape_CCS(seed_number):\n",
    "    np.random.seed(seed_number)\n",
    "    # create eigen_val noised weights\n",
    "    weights_coefficient_factor = np.round(0.35, num_digits_round) #np.round(np.random.uniform(0.2, 0.25), num_digits_round)\n",
    "    eigen_val_weights = np.random.randn(eigen_vec_num, 1)*weights_coefficient_factor*scale_factor\n",
    "    eigen_val_vec = eigen_val.reshape((eigen_val.shape[0], 1))\n",
    "\n",
    "    eigen_vec_multipliers = np.multiply(eigen_val_weights, eigen_val_vec)\n",
    "    added_deformation = eigen_vec @ eigen_vec_multipliers\n",
    "\n",
    "    # add the added_deformation to the mean shape\n",
    "    new_shape = mean_shape + added_deformation\n",
    "    new_shape_CCS = new_shape.reshape(-1,3)\n",
    "\n",
    "    return new_shape_CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new head instance and its relevant features\n",
    "def create_new_instance_coordinates_geodesic_distances_over_resolutions(current_instance_index):\n",
    "    instance_EEG_10_20_geodesic_distances = np.zeros((len(decimation_percentages), len(distances_to_find)))\n",
    "    instance_landmarks_coordinates_array = np.zeros((len(decimation_percentages), num_of_landmarks*3))\n",
    "\n",
    "    new_shape_original_density_CCS = get_new_shape_CCS(current_instance_index)\n",
    "\n",
    "    for decimation_index, decimation_percentage in enumerate(decimation_percentages):\n",
    "        if decimation_index==0:\n",
    "            landmarks_CCS = new_shape_original_density_CCS[landmarks, :]\n",
    "            landmarks_CCS_array = landmarks_CCS.reshape(1, -1).squeeze()\n",
    "            instance_landmarks_coordinates_array[decimation_index, :] = landmarks_CCS_array\n",
    "\n",
    "            new_shape_original_density_trimesh = trimesh.Trimesh(vertices=new_shape_original_density_CCS, faces=trilist, process=True)\n",
    "\n",
    "            current_shape_trimesh = new_shape_original_density_trimesh\n",
    "        \n",
    "            try:\n",
    "                current_trimesh_landmarks_indices = get_trimesh_indices(current_shape_trimesh, new_shape_original_density_CCS, landmarks)\n",
    "                current_indices = current_trimesh_landmarks_indices\n",
    "            except:\n",
    "                print(current_instance_index+\" instance failed\")\n",
    "            \n",
    "        instance_landmarks_coordinates_array[decimation_index, :] = np.array(current_shape_trimesh.vertices[current_indices]).ravel()\n",
    "\n",
    "        current_shape_graph = trimesh.graph.vertex_adjacency_graph(current_shape_trimesh)\n",
    "        current_shape_graph_weights = get_graph_weights(current_shape_graph, current_shape_trimesh)\n",
    "        \n",
    "    \n",
    "    for i, current_distance in enumerate(distances_to_find):\n",
    "        current_landmark_index_i = current_distance[0]\n",
    "        current_landmark_index_j = current_distance[1]\n",
    "        try:\n",
    "            current_geodesic_distance = nx.dijkstra_path_length(current_shape_graph_weights,\n",
    "                                                                current_landmark_index_i, \n",
    "                                                                current_landmark_index_j, \n",
    "                                                                weight='euclidean_distance')\n",
    "        except:\n",
    "            print(f\"{current_instance_index}, {decimation_percentage}, cranial landmarks {current_landmark_index_i, current_landmark_index_j} geodesic distance failed\")\n",
    "            current_geodesic_distance = 0\n",
    "\n",
    "        instance_EEG_10_20_geodesic_distances[0, i] = current_geodesic_distance\n",
    "    \n",
    "    return instance_landmarks_coordinates_array, instance_EEG_10_20_geodesic_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa8f98-7f87-4a03-8908-276b4cc55cab",
   "metadata": {},
   "source": [
    "# Get mean shape model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shape_trimesh = trimesh.Trimesh(vertices=mean_shape_CCS, faces=trilist, process=True)\n",
    "\n",
    "if decimation==True and choose_light_model==False:\n",
    "    mean_shape_trimesh.update_vertices(module_to_remove)\n",
    "\n",
    "original_num_of_faces = mean_shape_trimesh.faces.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da747713-a74d-4b2a-b63f-4b72899c165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if choose_light_model==True:\n",
    "    mean_trimesh_landmarks_indices = get_trimesh_indices(mean_shape_trimesh, mean_shape_CCS, landmarks)\n",
    "else:\n",
    "    mean_trimesh_landmarks_indices = get_trimesh_indices(mean_shape_trimesh, mean_shape_CCS[module_to_remove, :], landmarks)\n",
    "\n",
    "mean_shape_graph = trimesh.graph.vertex_adjacency_graph(mean_shape_trimesh)\n",
    "mean_shape_graph_weights = get_graph_weights(mean_shape_graph, mean_shape_trimesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88104a0-a129-4ddc-a943-0ad9b052a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shape_shortest_paths = []\n",
    "\n",
    "if calculate_rigid==True:\n",
    "    for i, current_landmark_index_i in enumerate(selected_EEG_10_20_indices):\n",
    "        for j, current_landmark_index_j in enumerate(selected_facial_indices):\n",
    "            current_geodesic_shortest_path = nx.dijkstra_path(mean_shape_graph_weights, \n",
    "                                                              mean_trimesh_landmarks_indices[current_landmark_index_i], \n",
    "                                                              mean_trimesh_landmarks_indices[current_landmark_index_j], \n",
    "                                                              weight='euclidean_distance')\n",
    "            mean_shape_shortest_paths.append(current_geodesic_shortest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611a330-20a9-4cb5-9e49-d8d7f9974bee",
   "metadata": {},
   "source": [
    "# Create and save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611ccb5-4778-4810-946e-dc4629151928",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_coordinates_array = np.zeros((num_of_instances, len(decimation_percentages), num_of_landmarks*3))\n",
    "geodesic_distances_array = np.zeros((num_of_instances, len(decimation_percentages), len(distances_to_find)))\n",
    "\n",
    "if calculate_rigid:\n",
    "    rigid_geodesic_distances = np.zeros((num_of_instances, len(selected_EEG_10_20_indices)*len(selected_facial_indices)))\n",
    "\n",
    "already_existing_instances = int(0e4)\n",
    "pool = mp.Pool(mp.cpu_count()-4) #mp.Pool(mp.cpu_count()-3) / mp.Pool(6)\n",
    "instances_results = []\n",
    "start_time = time.time()\n",
    "instances_results = pool.starmap(create_new_instance_coordinates_geodesic_distances_over_resolutions, \n",
    "                                 [(already_existing_instances+current_instance_index,) for current_instance_index in range(num_of_instances)])\n",
    "end_time = time.time()\n",
    "pool.close()\n",
    "\n",
    "for current_instance in range(num_of_instances):\n",
    "    landmarks_coordinates_array[current_instance, :, :] = instances_results[current_instance][0]\n",
    "    geodesic_distances_array[current_instance, :, :] = instances_results[current_instance][1]\n",
    "    #if calculate_rigid:\n",
    "    #    rigid_geodesic_distances[current_instance, :] = instances_results[current_instance][2]\n",
    "        \n",
    "print((end_time-start_time)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c792a-a960-45d5-98c9-8c39b04131aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c76a1-642a-4ad8-a324-3fd760f5e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_coordinates_array = landmarks_coordinates_array.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./datasets/TMS/3DMM/Head_instances/\"\n",
    "filetype = \".xlsx\"\n",
    "#filename = \"32_light_landmarks_geodesic_distances_of_facial_landmarks_over_resolutions_and_coordinates_05_24\"\n",
    "filename = \"dataset\"\n",
    "path = folder + filename + filetype\n",
    "\n",
    "if decimation==False:\n",
    "    if create_new==True:\n",
    "        df_all_landmarks_names = np.repeat(landmarks_names, 3)\n",
    "        df_coordinates_names = np.array(['x', 'y', 'z']*num_of_landmarks)\n",
    "        landmarks_coordinates_df = pd.DataFrame(data=landmarks_coordinates_array, columns=\n",
    "                                                pd.MultiIndex.from_tuples(zip(df_all_landmarks_names, df_coordinates_names)))\n",
    "\n",
    "        geodesic_distances_df = pd.DataFrame(data=geodesic_distances_array, columns=\n",
    "                                                pd.MultiIndex.from_tuples(zip(multiindex_up_names, multiindex_down_names)))\n",
    "\n",
    "        rigid_geodesic_distances_df = pd.DataFrame(data=rigid_geodesic_distances, columns=\n",
    "                                                   pd.MultiIndex.from_tuples(zip(multiindex_up_names, multiindex_down_names)))\n",
    "\n",
    "        # create a excel writer object to write into multiple sheets\n",
    "        with pd.ExcelWriter(path) as writer:\n",
    "            landmarks_coordinates_df.round(num_digits_round).to_excel(writer, sheet_name='coordinates', index=1)\n",
    "            geodesic_distances_df.round(num_digits_round).to_excel(writer, sheet_name='geodesic', index=2)\n",
    "            rigid_geodesic_distances_df.round(num_digits_round).to_excel(writer, sheet_name='rigid_geodesic', index=3)\n",
    "    else:\n",
    "        # read facial features coordinates from an excel file as multiindex\n",
    "        landmarks_coordinates_df = pd.read_excel(path, header=[0,1], index_col=0, sheet_name='coordinates')\n",
    "        landmarks_coordinates = landmarks_coordinates_df.to_numpy()#[:, 1:]\n",
    "\n",
    "        geodesic_distances_df = pd.read_excel(path, header=[0,1], index_col=0, sheet_name='geodesic')\n",
    "        geodesic_distances = geodesic_distances_df.to_numpy()#[:, 1:]\n",
    "\n",
    "        rigid_geodesic_distances_df = pd.read_excel(path, header=[0,1], index_col=0, sheet_name='rigid_geodesic')\n",
    "        rigid_geodesic_distances = rigid_geodesic_distances_df.to_numpy()#[:, 1:]\n",
    "\n",
    "else:\n",
    "    landmarks_coordinates_df = []\n",
    "    geodesic_distances_df = []\n",
    "    \n",
    "    if create_new==True:\n",
    "        df_all_landmarks_names = np.repeat(landmarks_names, 3)\n",
    "        df_coordinates_names = np.array(['x', 'y', 'z']*num_of_landmarks)\n",
    "        \n",
    "        for l in range(len(decimation_percentages)):\n",
    "            landmarks_coordinates_df.append(pd.DataFrame(data=landmarks_coordinates_array, columns=\n",
    "                                                    pd.MultiIndex.from_tuples(zip(df_all_landmarks_names, df_coordinates_names))))\n",
    "            \n",
    "            geodesic_distances_df.append(pd.DataFrame(data=geodesic_distances_array[:, l, :], columns=\n",
    "                                                    #pd.MultiIndex.from_tuples(zip(multiindex_up_names, multiindex_down_names))))\n",
    "                                                      distances_to_find_names))\n",
    "            \n",
    "        # create a excel writer object to write into multiple sheets\n",
    "        with pd.ExcelWriter(path) as writer:\n",
    "            added_index = 1\n",
    "            \n",
    "            if calculate_rigid:\n",
    "                rigid_geodesic_distances_df.round(num_digits_round).to_excel(writer, sheet_name=('rigid_geodesic_'+str(decimation_percentages[0])), index=1)\n",
    "                added_index+=1\n",
    "            for l in range(len(decimation_percentages)):\n",
    "                landmarks_coordinates_df[l].round(num_digits_round).to_excel(writer, sheet_name=('coordinates_'+str(decimation_percentages[l])), index=2*l+added_index)\n",
    "                geodesic_distances_df[l].round(num_digits_round).to_excel(writer, sheet_name=('geodesic_'+str(decimation_percentages[l])), index=2*l+added_index+1)\n",
    "                \n",
    "    else:\n",
    "        # read facial features coordinates from an excel file as multiindex\n",
    "        if calculate_rigid:\n",
    "            rigid_geodesic_distances_df = pd.read_excel(path, header=[0,1], index_col=0, sheet_name=('rigid_geodesic_'+str(decimation_percentages[0])))\n",
    "            rigid_geodesic_distances = rigid_geodesic_distances_df.to_numpy()#[:, 1:]\n",
    "        \n",
    "        for l in range(len(decimation_percentages)):\n",
    "            landmarks_coordinates_df[l] = pd.read_excel(path, header=[0,1], index_col=0, sheet_name=('coordinates_' + str(decimation_percentages[l])))\n",
    "            landmarks_coordinates[l] = landmarks_coordinates_df[l].to_numpy()#[:, 1:]\n",
    "            \n",
    "            geodesic_distances_df[l] = pd.read_excel(path, header=[0,1], index_col=0, sheet_name=('geodesic_' + str(decimation_percentages[l])))\n",
    "            geodesic_distances[l] = geodesic_distances_df[l].to_numpy()#[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46076215-d619-4781-9296-c04bd6baae01",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879aeff-2eb2-4038-a6b2-b43ba2f867ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "if 1:\n",
    "    from datetime import datetime\n",
    "    timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "    \n",
    "    folder = \"./datasets/TMS/3DMM/Head_instances/\"\n",
    "    filetype = \".xlsx\"\n",
    "    \n",
    "    merged_filename = \"coordintaes_geodesic_distances_30_08\"\n",
    "    merged_path = folder + merged_filename + filetype\n",
    "    \n",
    "    first_filename = \"81028\"\n",
    "    first_path = folder + first_filename + filetype\n",
    "    \n",
    "    second_filename = \"08_29\"\n",
    "    second_path = folder + second_filename + filetype\n",
    "\n",
    "    first_landmarks_coordinates_df = []\n",
    "    first_geodesic_distances = []\n",
    "\n",
    "    second_landmarks_coordinates_df = []\n",
    "    second_geodesic_distances = []\n",
    "\n",
    "    merged_landmarks_coordinates_df = []\n",
    "    merged_geodesic_distances_df = []\n",
    "\n",
    "    for l in range(len(decimation_percentages)):\n",
    "        first_landmarks_coordinates_df.append(pd.read_excel(first_path, header=[0,1], index_col=0, sheet_name=('coordinates_' + str(decimation_percentages[l]))))\n",
    "        current_first_geodesic_distances_df = pd.read_excel(first_path, header=[0,1], index_col=0, sheet_name=('geodesic_' + str(decimation_percentages[l])))\n",
    "        first_geodesic_distances.append(np.asarray(current_first_geodesic_distances_df))\n",
    "\n",
    "        first_num_of_instances = first_landmarks_coordinates_df[l].shape[0]\n",
    "\n",
    "        second_landmarks_coordinates_df.append(pd.read_excel(second_path, header=[0,1], index_col=0, sheet_name=('coordinates_' + str(decimation_percentages[l]))))\n",
    "        current_second_geodesic_distances_df = pd.read_excel(second_path, header=[0,1], index_col=0, sheet_name=('geodesic_' + str(decimation_percentages[l])))\n",
    "        second_geodesic_distances.append(np.asarray(current_second_geodesic_distances_df))\n",
    "\n",
    "        second_num_of_instances = second_landmarks_coordinates_df[l].shape[0]\n",
    "\n",
    "        merged_landmarks_coordinates_df.append(pd.concat([first_landmarks_coordinates_df[l], second_landmarks_coordinates_df[l]]))\n",
    "        merged_landmarks_coordinates_df[l].index = np.arange(0, first_num_of_instances+second_num_of_instances)\n",
    "\n",
    "        current_merged_geodesic_distances_df = pd.DataFrame(data=np.concatenate((\n",
    "            first_geodesic_distances[l], second_geodesic_distances[l])), columns=pd.MultiIndex.from_tuples(\n",
    "            zip(multiindex_up_names, multiindex_down_names)))\n",
    "        merged_geodesic_distances_df.append(current_merged_geodesic_distances_df)\n",
    "\n",
    "    with pd.ExcelWriter(merged_path) as writer:\n",
    "        added_index = 1\n",
    "\n",
    "        if calculate_rigid:\n",
    "            rigid_geodesic_distances_df.round(num_digits_round).to_excel(writer, sheet_name=('rigid_geodesic_'+str(decimation_percentages[0])), index=1)\n",
    "            added_index+=1\n",
    "        for l in range(len(decimation_percentages)):\n",
    "            merged_landmarks_coordinates_df[l].round(num_digits_round).to_excel(writer, sheet_name=('coordinates_'+str(decimation_percentages[l])), index=2*l+added_index)\n",
    "            merged_geodesic_distances_df[l].round(num_digits_round).to_excel(writer, sheet_name=('geodesic_'+str(decimation_percentages[l])), index=2*l+added_index+1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
