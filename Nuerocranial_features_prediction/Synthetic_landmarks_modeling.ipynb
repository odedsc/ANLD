{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89b7fd0-4540-4cfc-a1be-be0a5917e931",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading and disaplying the 3DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime\n",
    "import plotly.io as pio\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109a8c8-9e20-4767-9008-c5694e3ee54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_folder = \"./datasets/TMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7dba2-17ff-4423-89ee-a9f6c1a407b3",
   "metadata": {},
   "source": [
    "## Original model including everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbc6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "models_folder = media_folder+'/3DMM/UHM_models/'\n",
    "currnet_model = 'UHM' \n",
    "\n",
    "# UHM model with all the components fused together (i.e. ears, inner mouth, and teeth)\n",
    "model_name = 'head_model_global_align'\n",
    "\n",
    "model_file = open(models_folder + model_name + '.pkl', 'rb')\n",
    "model_dict = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning coordinates system to be in millimeter units\n",
    "scale_factor = 100 \n",
    "\n",
    "# get model parameteres\n",
    "mean_shape = scale_factor*model_dict['Mean']\n",
    "mean_shape_CCS = mean_shape.reshape(-1,3)\n",
    "eigen_vec = model_dict['Eigenvectors']\n",
    "eigen_vec_num =  model_dict['Eigenvectors'].shape[1]\n",
    "eigen_val = model_dict['EigenValues']\n",
    "trilist = model_dict['Trilist']\n",
    "vertices_num = model_dict['Number_of_vertices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules (landmarks and masks)\n",
    "modules_folder = models_folder + '/Landmarks and masks/'\n",
    "\n",
    "modules_to_load = ['68_land_idxs'] # EEG_10_20_full_model / '49_plus_ears_land_idxs' / '68_land_idxs'\n",
    "\n",
    "landmarks = []\n",
    "landmarks_names = []\n",
    "landmarks_groups = []\n",
    "\n",
    "for currnet_module_name in modules_to_load:\n",
    "    module_file = open(modules_folder + currnet_module_name + '.pkl', 'rb')\n",
    "    currnet_module = pickle.load(module_file)\n",
    "    module_file.close()\n",
    "    if currnet_module_name=='EEG_10_20':\n",
    "        currnet_module_names = list(currnet_module.keys())\n",
    "        currnet_module = np.asarray(list(currnet_module.values()))\n",
    "    else:\n",
    "        currnet_module_names = list(map(str, 1+np.arange(len(currnet_module))))\n",
    "        \n",
    "    landmarks.append(currnet_module)\n",
    "    landmarks_names.append(currnet_module_names)\n",
    "    landmarks_groups.append(np.arange(len(currnet_module)))\n",
    "\n",
    "# turn list of lists into one list\n",
    "landmarks = [item for items in landmarks for item in items]\n",
    "landmarks_names = [item for items in landmarks_names for item in items]\n",
    "\n",
    "num_of_landmarks = len(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3d161-79d4-4b57-bfb7-1637689b02d7",
   "metadata": {},
   "source": [
    "## Lighter model including everything but eyes, teeth and inner mouth cavity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6328036-ddc5-43cf-b6e4-b2796579a647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "light_models_folder = media_folder+'/3DMM/UHM_models/'\n",
    "light_currnet_model = 'UHM' \n",
    "\n",
    "# UHM model with all the components fused together (i.e. ears, inner mouth, and teeth)\n",
    "light_model_name = 'head_model_global_align_no_mouth_and_eyes'\n",
    "\n",
    "light_model_file = open(light_models_folder + light_model_name + '.pkl', 'rb')\n",
    "light_model_dict = pickle.load(light_model_file)\n",
    "light_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3054a7-0bf9-48cf-b1a7-99fa02f11efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning cartesian coordinates system to be in millimeter units\n",
    "light_scale_factor = 100\n",
    "\n",
    "# get model parameteres\n",
    "light_mean_shape = light_scale_factor*light_model_dict['Mean']\n",
    "light_mean_shape_CCS = light_mean_shape.reshape(-1,3)\n",
    "light_eigen_vec = light_model_dict['Eigenvectors']\n",
    "light_eigen_vec_num =  light_model_dict['Eigenvectors'].shape[1]\n",
    "light_eigen_val = light_model_dict['EigenValues']\n",
    "light_trilist = light_model_dict['Trilist']\n",
    "light_vertices_num = light_model_dict['Number_of_vertices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6be30-d113-4343-b12f-517e24e459e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules (landmarks and masks)\n",
    "modules_folder = models_folder + '/Landmarks and masks/'\n",
    "\n",
    "modules_to_load = ['EEG_10_20'] # EEG_10_20 / '49_plus_ears_land_idxs' / '68_land_idxs'\n",
    "\n",
    "light_landmarks = []\n",
    "light_landmarks_names = []\n",
    "light_landmarks_groups = []\n",
    "\n",
    "for currnet_module_name in modules_to_load:\n",
    "    module_file = open(modules_folder + currnet_module_name + '.pkl', 'rb')\n",
    "    currnet_module = pickle.load(module_file)\n",
    "    module_file.close()\n",
    "    if currnet_module_name=='EEG_10_20':\n",
    "        currnet_module_names = list(currnet_module.keys())\n",
    "        currnet_module = np.asarray(list(currnet_module.values()))\n",
    "    else:\n",
    "        currnet_module_names = list(map(str, 1+np.arange(len(currnet_module))))\n",
    "        \n",
    "    light_landmarks.append(currnet_module)\n",
    "    light_landmarks_names.append(currnet_module_names)\n",
    "    light_landmarks_groups.append(np.arange(len(currnet_module)))\n",
    "\n",
    "# turn list of lists into one list\n",
    "light_landmarks = [item for items in light_landmarks for item in items]\n",
    "light_landmarks_names = [item for items in light_landmarks_names for item in items]\n",
    "\n",
    "num_of_light_landmarks = len(light_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e3cbd-5c01-47a2-99a8-59d172e4ed1a",
   "metadata": {},
   "source": [
    "## Matching landmark indices between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcca4c0-7085-49fe-a635-c5f4c42646cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_facial_landmarks = landmarks\n",
    "\n",
    "for current_landmark_index, current_landmark_vertex in enumerate(landmarks):\n",
    "    original_model_coordinates = mean_shape_CCS[current_landmark_vertex]\n",
    "    new_model_vertex_diffs = np.linalg.norm(light_mean_shape_CCS-original_model_coordinates, axis=1)\n",
    "    light_facial_landmarks[current_landmark_index] = np.argmin(new_model_vertex_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccb179-9d58-4b82-88e6-8c07aee0eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_nasion = 52241\n",
    "light_inion = 36323"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dd8ed-ce45-4cab-9609-d8f73bc7edba",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3828c-2dee-4ac2-ac5b-fd8b7b18bc95",
   "metadata": {},
   "source": [
    "## Model Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f0ae0-e750-441c-9f05-a963acdba5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "choose_light_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db86eb-3571-4392-8a28-07aff4a9cc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if choose_light_model:\n",
    "    landmarks = np.concatenate((light_landmarks, light_facial_landmarks, [light_nasion, light_inion]))\n",
    "    landmarks_names = list(np.concatenate((light_landmarks_names, landmarks_names, ['nasion', 'inion'])))\n",
    "    num_of_landmarks = len(landmarks_names)\n",
    "    mean_shape = light_mean_shape\n",
    "    mean_shape_CCS = light_mean_shape_CCS\n",
    "    eigen_vec = light_eigen_vec\n",
    "    eigen_vec_num =  light_eigen_vec_num\n",
    "    eigen_val = light_eigen_val\n",
    "    trilist = light_trilist\n",
    "    vertices_num = light_vertices_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68caad-7ef7-4ab4-95f5-0cd83246d41a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shape_trimesh = trimesh.Trimesh(vertices=mean_shape_CCS, faces=trilist, process=True)\n",
    "num_digits_round=4\n",
    "n_jobs_num=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfad12-7777-41e0-9a8e-05b8334af447",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_landmarks_names = np.array([37, 40, 43, 46, 49, 55, 31, 9])\n",
    "rigid_facial_landmarks_names = np.array([37, 40, 43, 46, 28, 1, 17])\n",
    "\n",
    "center_of_the_eyebrows = np.array([20, 25])\n",
    "corners_of_the_eyebrows = np.array([18, 22, 23, 27])\n",
    "corners_of_the_eyes = np.array([37, 40, 43, 46])\n",
    "sides_of_the_face = np.array([1, 17])\n",
    "nose_bone = np.array([28, 31])\n",
    "lower_nose = np.array([32, 34, 36])\n",
    "corners_of_the_mouth = np.array([49, 55])\n",
    "chin = np.array([9])\n",
    "\n",
    "facial_landmarks = np.concatenate((center_of_the_eyebrows, corners_of_the_eyebrows, corners_of_the_eyes, sides_of_the_face, nose_bone, lower_nose,\n",
    "                                   corners_of_the_mouth, chin))\n",
    "selected_facial_indices = np.sort(facial_landmarks+num_of_light_landmarks-1)\n",
    "\n",
    "selected_EEG_10_20_landmark_names = light_landmarks_names\n",
    "selected_EEG_10_20_indices = []\n",
    "for current_index, current_landmark_name in enumerate(selected_EEG_10_20_landmark_names):\n",
    "    selected_EEG_10_20_indices.append(landmarks_names.index(current_landmark_name))\n",
    "selected_EEG_10_20_indices = np.asarray(selected_EEG_10_20_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88714baa-8021-4930-a592-6bfc11a73270",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_landmarks = ['1', '2', '3', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
    "                   '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42',\n",
    "                   '43', '44', '45', '46', '47', '48', '69', '70']\n",
    "input_landmarks = [int(current_landmark) for current_landmark in input_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92549f2c-63c7-47a2-8634-4d6344e5c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = np.concatenate((selected_EEG_10_20_indices, 20+np.array(input_landmarks)))\n",
    "selected_indices_names = np.take(landmarks_names, selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fcc0b-9003-4a90-9cd3-867b917f447a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_sets = {}\n",
    "feature_sets['eye corners & eyebrow corners'] = np.concatenate((corners_of_the_eyebrows, corners_of_the_eyes))\n",
    "feature_sets['eye corners & eyebrow centers'] = np.concatenate((center_of_the_eyebrows, corners_of_the_eyes))\n",
    "feature_sets['eye corners & eyebrow corners and center'] = np.concatenate((center_of_the_eyebrows, corners_of_the_eyebrows, corners_of_the_eyes))\n",
    "feature_sets['eye corners & nose bone'] = np.concatenate((corners_of_the_eyes, nose_bone))\n",
    "feature_sets['nose bone & lower nose'] = np.concatenate((nose_bone, lower_nose))\n",
    "feature_sets['input_landmarks'] = np.array(input_landmarks[:-2])\n",
    "\n",
    "for current_key in feature_sets:\n",
    "    feature_sets[current_key] = feature_sets[current_key]+num_of_light_landmarks-1\n",
    "    feature_sets[current_key] = list(map(str, feature_sets[current_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4465a-8ba8-48d6-ab59-5530d56b5edb",
   "metadata": {},
   "source": [
    "## Pytorch models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15c0a7-b505-4385-9466-e73412123e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f5c94-69cc-481a-8aeb-d6a81b36b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_3(nn.Module):\n",
    "    def __init__(self, inputs_size, output_size, first_hidden_layer_size=2**7, second_hidden_layer_size=2**5, third_hidden_layer_size=2**3):\n",
    "        super().__init__()\n",
    "        #self.layers = nn.Sequential(\n",
    "        self.inputs_size = inputs_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(inputs_size, first_hidden_layer_size)\n",
    "        self.bn1 = nn.BatchNorm1d(first_hidden_layer_size)\n",
    "        self.d1 = nn.Dropout(p=0.3, inplace=False)\n",
    "        \n",
    "        self.fc2 = nn.Linear(first_hidden_layer_size, second_hidden_layer_size)\n",
    "        self.bn2 = nn.BatchNorm1d(second_hidden_layer_size)\n",
    "        self.d2 = nn.Dropout(p=0.2, inplace=False)\n",
    "        \n",
    "        self.fc3 = nn.Linear(second_hidden_layer_size, third_hidden_layer_size)\n",
    "        self.bn3 = nn.BatchNorm1d(third_hidden_layer_size)\n",
    "        self.d3 = nn.Dropout(p=0.2, inplace=False)\n",
    "        \n",
    "        self.fc4 = nn.Linear(third_hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x) #torch.tanh(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x) #torch.tanh(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x) #torch.tanh(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3da6b9-bcf9-428b-aea2-5c66ecb65daf",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3aace7-6f95-4180-9576-e17253befa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, dataloader, loss_function, optimizer, output_size, mode):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    current_loss = 0.0\n",
    "    if mode=='train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "        \n",
    "        targets = targets.reshape((targets.shape[0], output_size))\n",
    "        if mode=='train':\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "        # Perform forward pass\n",
    "        current_outputs = model(inputs)\n",
    "        # Compute loss\n",
    "        loss = loss_function(current_outputs, targets)\n",
    "        if mode=='test':\n",
    "            if i==0:\n",
    "                outputs = current_outputs\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, current_outputs), dim=0)\n",
    "        if mode=='train':\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "        #if mode!='test':\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "    if mode=='test':\n",
    "        return current_loss, model, outputs\n",
    "    else:\n",
    "        return current_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cc910-a4a2-4517-bf22-aefd0f9ee75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_choose_and_predict(regressor_name, X_train, y_train, X_test, y_test, n_jobs_num=n_jobs_num):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f'Using {torch.cuda.get_device_name(0)}')\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print('Using CPU')\n",
    "            \n",
    "    if regressor_name=='pytorch_MLP' or regressor_name=='MLP':\n",
    "        \n",
    "        test_losses = []\n",
    "        \n",
    "        validation = False # False / True\n",
    "        validation_losses = []\n",
    "        \n",
    "        batch_size = 2**3\n",
    "        lr = 5e-3\n",
    "\n",
    "        triggered = False\n",
    "\n",
    "        if y_test.shape[1]>1:\n",
    "            output_size = 3\n",
    "        else:\n",
    "            output_size = 1\n",
    "        \n",
    "        train_target = torch.tensor(y_train.astype(np.float32)) \n",
    "        train_input = torch.tensor(X_train.astype(np.float32))\n",
    "        \n",
    "        train_tensor = TensorDataset(train_input, train_target)\n",
    "                \n",
    "        if validation==True:\n",
    "            train_set_percentage = 0.8\n",
    "            lowest_validation_loss = 1e6\n",
    "\n",
    "            train_set_size = int(np.round(train_set_percentage*train_input.shape[0]))\n",
    "            valid_set_size = train_input.shape[0]-train_set_size\n",
    "\n",
    "            train_tensor, valid_tensor = random_split(train_tensor, [train_set_size, valid_set_size])\n",
    "\n",
    "            validloader = DataLoader(dataset=valid_tensor, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        trainloader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        test_input = torch.tensor(X_test.astype(np.float32))\n",
    "        test_target = torch.tensor(y_test.astype(np.float32))\n",
    "\n",
    "        test_tensor = TensorDataset(test_input, test_target) \n",
    "        testloader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        loss_function = nn.MSELoss()\n",
    "        if output_size>1:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "            print('MLP')\n",
    "        else:\n",
    "            mlp = MLP_nn(X_train.shape[1], output_size)\n",
    "            mlp = nn.DataParallel(mlp, device_ids=[0])\n",
    "            print('MLP')\n",
    "        optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)#, verbose=True)\n",
    "\n",
    "        # Run the training loop\n",
    "        for epoch in range(1000):\n",
    "            #print(epoch)\n",
    "            if triggered==True:\n",
    "                continue\n",
    "                \n",
    "            train_loss, regressor_model = run_model(mlp.to(device), trainloader, loss_function, optimizer, output_size, mode='train')\n",
    "            \n",
    "            if validation==True:\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, _ = run_model(mlp.to(device), validloader, loss_function, optimizer, output_size, mode='validation')\n",
    "                    validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    last_validation_loss = validation_loss\n",
    "                    if validation_loss<lowest_validation_loss:\n",
    "                        lowest_validation_loss=validation_loss\n",
    "                        lowest_validation_loss_model=copy.deepcopy(mlp)\n",
    "                        lowest_validation_loss_epoch=epoch\n",
    "\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Disable grad\n",
    "            with torch.no_grad():\n",
    "                test_loss , _, current_predictions = run_model(mlp.to(device), testloader, loss_function, optimizer, output_size, mode='test')\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "        if validation==True:\n",
    "            print(f\"lowest_validation_loss_epoch is {lowest_validation_loss_epoch}\")\n",
    "        else:\n",
    "            print(f\"lowest_validation_loss_epoch is {epoch}\")\n",
    "        \n",
    "        if validation==True:\n",
    "            with torch.no_grad():\n",
    "                _ , _, lowest_validation_loss_predictions = run_model(lowest_validation_loss_model.to(device), testloader, loss_function, optimizer, output_size, mode='test')\n",
    "    \n",
    "    if validation==True:\n",
    "        return lowest_validation_loss_predictions, lowest_validation_loss_model, validation_losses, test_losses\n",
    "    else:\n",
    "        return current_predictions, regressor_model, validation_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae22bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_by_coordinates_regression(df, train_indices, test_indices, features_to_use, predicted_feature_index, \n",
    "                                         regressor_name='MLP'):\n",
    "    y_train = np.asarray(df.loc[train_indices, ([landmarks_names[predicted_feature_index]])])#.reshape((-1, 3))\n",
    "    y_test = np.asarray(df.loc[test_indices, ([landmarks_names[predicted_feature_index]])])#.reshape((-1, 3))\n",
    "    \n",
    "    features_to_use = np.take(np.asarray(landmarks_names), features_to_use)\n",
    "    X_train = np.asarray(df.loc[train_indices, features_to_use])\n",
    "    X_test = np.asarray(df.loc[test_indices, features_to_use])\n",
    "\n",
    "    means = np.mean(X_train, axis=0)\n",
    "    \n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    X_test = X_standard_scaler.transform(X_test)\n",
    "    \n",
    "    current_predictions, regressor_model, validation_losses, test_losses = model_choose_and_predict(regressor_name, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    euclidean_errors = np.linalg.norm(current_predictions.to(\"cpu\")-y_test, axis=1)\n",
    "    squared_euclidean_errors = euclidean_errors**2\n",
    "    mean_squared_euclidean_error = np.mean(squared_euclidean_errors)\n",
    "    std_euclidean_errors = np.std(euclidean_errors)\n",
    "   \n",
    "    return regressor_model, mean_squared_euclidean_error, squared_euclidean_errors, std_euclidean_errors, X_standard_scaler, means, validation_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe03d9-f149-4c81-bda4-6a6ea160c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_by_coordinates_and_geodesic_distance_regression(coordinates_df, geodesic_df, train_indices, test_indices, features_to_use,\n",
    "                                                                predicted_feature_index, regressor_name='MLP'):\n",
    "    y_train = np.asarray(coordinates_df.loc[train_indices, ([landmarks_names[predicted_feature_index]])])\n",
    "    y_test = np.asarray(coordinates_df.loc[test_indices, ([landmarks_names[predicted_feature_index]])])\n",
    "    \n",
    "    features_to_use = np.take(np.asarray(landmarks_names), features_to_use)\n",
    "    \n",
    "    coordinates_train = np.asarray(coordinates_df.loc[train_indices, features_to_use])\n",
    "    coordinates_test = np.asarray(coordinates_df.loc[test_indices, features_to_use])\n",
    "    \n",
    "    geodesic_data = np.zeros((geodesic_df.shape[0], 3))\n",
    "    for i in range(geodesic_df.shape[0]):\n",
    "        geodesic_data[i, :2] = np.array(geodesic_df.iloc[i, :2])\n",
    "        geodesic_data[i, 2] = np.sum(np.array(geodesic_df.iloc[i, 2:]))\n",
    "        \n",
    "    geodesic_train = geodesic_data[train_indices, :]\n",
    "    geodesic_test = geodesic_data[test_indices, :]\n",
    "\n",
    "    X_train = np.concatenate((coordinates_train, geodesic_train), axis=1)\n",
    "    print(coordinates_train.shape, geodesic_train.shape, X_train.shape)\n",
    "    X_test = np.concatenate((coordinates_test, geodesic_test), axis=1)\n",
    "    \n",
    "    X_standard_scaler = StandardScaler()\n",
    "    X_train = X_standard_scaler.fit_transform(X_train)\n",
    "    X_test = X_standard_scaler.transform(X_test)\n",
    "    \n",
    "    means = np.mean(X_train, axis=0)\n",
    "       \n",
    "    current_predictions, regressor_model, validation_losses, test_losses = model_choose_and_predict(regressor_name, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    euclidean_errors = np.linalg.norm(current_predictions.to(\"cpu\")-y_test, axis=1)\n",
    "    squared_euclidean_errors = euclidean_errors**2\n",
    "    mean_squared_euclidean_error = np.mean(squared_euclidean_errors)\n",
    "    std_euclidean_errors = np.std(euclidean_errors)\n",
    "   \n",
    "    return regressor_model, mean_squared_euclidean_error, squared_euclidean_errors, std_euclidean_errors, X_standard_scaler, means, validation_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead3491-14b7-4bfb-a6df-9733af7dc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_arrays_loader(regressor_name, experiment_number):\n",
    "    array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/\"+regressor_name+\"/\"\n",
    "\n",
    "    files = []\n",
    "    for current_file in os.listdir(array_folder):\n",
    "        if current_file.startswith((\"experiment_\"+str(experiment_number))) and current_file.endswith(\".npy\"):\n",
    "            files.append(current_file)\n",
    "\n",
    "    files.sort()\n",
    "    last_file_created = files[-1]\n",
    "\n",
    "    with open(last_file_created, 'rb') as file:\n",
    "        MSE_array = np.load(file)\n",
    "        std_array = np.load(file)\n",
    "        \n",
    "    return MSE_array, std_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4eddf2-420f-4650-a2f0-d3158a838c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_saver(experiment_number, arrays_list, regressor, save_figures=True, save_arrays=True):\n",
    "    if regressor=='MLP' or regressor=='pytorch_MLP':\n",
    "        regressor_name = 'pytorch_MLP'\n",
    "\n",
    "    timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "    figure_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/\"+regressor_name+\"/std/\"\n",
    "    figure_filetype = \".jpg\"\n",
    "    figure_filename = \"experiment_\"+str(experiment_number)+\"_\"+timestamp_string+\"_\"+filename\n",
    "\n",
    "    figure_path = figure_folder + figure_filename + figure_filetype\n",
    "\n",
    "    fig.layout.title = \"\"\n",
    "\n",
    "    fig.update_layout(yaxis = dict(dtick = 0.25))  \n",
    "    fig.update_layout(yaxis_range=[0,3])\n",
    "\n",
    "    if save_figures:\n",
    "        pio.write_image(fig, figure_path, scale=5)\n",
    "\n",
    "    figure_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/\"+regressor_name+\"/no_std/\"\n",
    "    figure_filetype = \".jpg\"\n",
    "    figure_filename = \"experiment_\"+str(experiment_number)+\"_\"+timestamp_string+\"_\"+filename\n",
    "\n",
    "    figure_path = figure_folder + figure_filename + figure_filetype\n",
    "    \n",
    "    no_std_traces = []\n",
    "    \n",
    "    for current_trace_index in range(len(fig.data)):\n",
    "        if np.mod(current_trace_index, 3)==0:\n",
    "            no_std_traces.append(fig.data[current_trace_index])\n",
    "\n",
    "    fig.data = no_std_traces\n",
    "\n",
    "    fig.update_layout(yaxis_range=[0,2])\n",
    "\n",
    "    if save_figures:\n",
    "        pio.write_image(fig, figure_path, scale=5)\n",
    "    \n",
    "    if save_arrays:\n",
    "        array_folder = media_folder+\"/Cranium_estimation_paper/Figures/Predictions/\"+regressor_name+\"/\"\n",
    "        array_filetype = '.npy'\n",
    "\n",
    "        array_path = array_folder + figure_filename + array_filetype\n",
    "\n",
    "        with open(array_path, 'wb') as file:\n",
    "            for i in range(len(arrays_list)):\n",
    "                np.save(file, arrays_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d21e8-fd50-476a-bffa-fa6dfbbe422d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading instances dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d73e8-fac7-4e3f-bae1-729ee8a3d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = media_folder+\"/3DMM/Head_instances/\"\n",
    "filetype = \".xlsx\"\n",
    "fixed_filename = \"dataset\" \n",
    "fixed_path = folder + fixed_filename + filetype\n",
    "\n",
    "excel_file = pd.ExcelFile(fixed_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e020d27-6556-4a91-94c6-abc3eb9bea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read facial features coordinates from an excel file as multiindex\n",
    "fixed_landmarks_coordinates_df=pd.read_excel(fixed_path, header=[0,1], index_col=0, sheet_name=('coordinates_' + str(1)), engine='openpyxl')\n",
    "fixed_geodesic_distances_df=pd.read_excel(fixed_path, index_col=0, sheet_name=('geodesic_' + str(1)), engine='openpyxl')\n",
    "\n",
    "fixed_num_of_instances = fixed_landmarks_coordinates_df.shape[0]\n",
    "fixed_landmarks_coordinates_df = fixed_landmarks_coordinates_df/1000\n",
    "fixed_geodesic_distances_df = fixed_geodesic_distances_df/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177ef6a-1435-4294-acda-629e3987f120",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Landmark Predictions - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8e116-9b20-419e-9e65-971fa265049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_nn = MLP_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625f733-8149-4ee2-bb75-8464d274323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figures = False\n",
    "save_arrays = True\n",
    "load_arrays = False\n",
    "save_model = True\n",
    "regressor_name='pytorch_MLP'\n",
    "regressor_models_folder = media_folder+'/3DMM/Trained_models/' + regressor_name + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84b979-e3f8-4f23-a5e8-0f95f4a89a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "test_size=0.0001\n",
    "\n",
    "train_indices = np.sort(np.random.choice(range(fixed_landmarks_coordinates_df.shape[0]),\n",
    "                                         #1, replace=False))\n",
    "                                 int(fixed_landmarks_coordinates_df.shape[0]*(1-test_size)), replace=False))\n",
    "test_indices = np.setdiff1d(np.arange(fixed_landmarks_coordinates_df.shape[0]), train_indices)#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051fbda-c096-4bef-8255-50a53a9ca007",
   "metadata": {},
   "source": [
    "## Predict coordinates using coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc5438-0233-4af4-aade-73f6402d22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 1\n",
    "\n",
    "experiment_model_filename = 'Coordinates/'+MLP_folder+'3DMM/'\n",
    "experiment_model_path = regressor_models_folder + experiment_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437fdc8-a740-4bfb-88f6-2eaf3d5f9492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "if load_arrays==False:\n",
    "    coordinates_coordinates_resolutions_MSE_array = np.zeros((len(selected_EEG_10_20_indices),))\n",
    "    coordinates_coordinates_resolutions_std_array = np.zeros((len(selected_EEG_10_20_indices),))\n",
    "    validation_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    feature_set_index=len(list(feature_sets.keys()))-1\n",
    "\n",
    "    current_features = feature_sets[list(feature_sets.keys())[feature_set_index]]\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate((np.take(landmarks_names, selected_EEG_10_20_indices))):\n",
    "        model, coordinates_coordinates_resolutions_MSE_array[desired_landmark_index], _, coordinates_coordinates_resolutions_std_array[\n",
    "            desired_landmark_index], scaler, means, validation_loss, test_loss = coordinates_by_coordinates_regression(\n",
    "            fixed_landmarks_coordinates_df, train_indices, test_indices, current_features, desired_landmark_index, regressor_name)\n",
    "\n",
    "        validation_losses.append(validation_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(1000*np.sqrt(coordinates_coordinates_resolutions_MSE_array[desired_landmark_index]))\n",
    "\n",
    "        if save_model:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "            pickle.dump(scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"landmark_names_being_used: {list(np.sort(np.array(list(map(int, current_features)))-20))}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"means: {means}\",\n",
    "                f\"model: {model}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        print(f\"Finished {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_indices)}\")\n",
    "\n",
    "else:\n",
    "    coordinates_coordinates_resolutions_MSE_array, coordinates_coordinates_resolutions_std_array = experiment_arrays_loader(regressor_name, experiment_number)\n",
    "    save_arrays = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09eb75-68d4-44b2-b9a6-8d3f62ad63b3",
   "metadata": {},
   "source": [
    "## Predict coordinates using coordinates and geodesic distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6abf65-2c51-49fa-86a2-a03fb017477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 2\n",
    "\n",
    "experiment_model_filename = 'Coordinates_Geodesic/'+MLP_folder+'3DMM/'\n",
    "\n",
    "experiment_model_path = regressor_models_folder + experiment_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90204f1-9606-419a-bcc9-cf3f6af8db52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "if load_arrays==False:\n",
    "    coordinates_coordinates_resolutions_MSE_array = np.zeros((len(selected_EEG_10_20_indices),))\n",
    "    coordinates_coordinates_resolutions_std_array = np.zeros((len(selected_EEG_10_20_indices),))\n",
    "    validation_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    feature_set_index=len(list(feature_sets.keys()))-1\n",
    "\n",
    "    current_features = feature_sets[list(feature_sets.keys())[feature_set_index]]\n",
    "    for desired_landmark_index, desired_landmark_name in enumerate((np.take(landmarks_names, selected_EEG_10_20_indices))):\n",
    "        \n",
    "        model, coordinates_coordinates_resolutions_MSE_array[desired_landmark_index], _, coordinates_coordinates_resolutions_std_array[\n",
    "            desired_landmark_index], scaler, means, validation_loss, test_loss = coordinates_by_coordinates_and_geodesic_distance_regression(\n",
    "            fixed_landmarks_coordinates_df, fixed_geodesic_distances_df, train_indices, test_indices,current_features,\n",
    "            desired_landmark_index, regressor_name)\n",
    "\n",
    "        validation_losses.append(validation_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(1000*np.sqrt(coordinates_coordinates_resolutions_MSE_array[desired_landmark_index]))\n",
    "\n",
    "        if save_model:\n",
    "            timestamp_string = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "            timestamp_string = timestamp_string.replace('_2022_', '_22_')\n",
    "\n",
    "            torch.save(model.state_dict(), experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_model')\n",
    "            pickle.dump(scaler, open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_scaler.pkl', 'wb'))\n",
    "            documentation = [\n",
    "                f\"predicted_landmark_name: {desired_landmark_name}\",\n",
    "                f\"landmark_names_being_used: {list(np.sort(np.array(list(map(int, current_features)))-20))}\",\n",
    "                f\"number_of_training_samples: {train_indices.size}\",\n",
    "                f\"means: {means}\",\n",
    "                f\"model: {model}\",\n",
    "            ]\n",
    "            with open(experiment_model_path+timestamp_string+'_'+desired_landmark_name+'_documentation.txt' , \"w\") as txt_file:\n",
    "                txt_file.write(\"\\n\".join(documentation))\n",
    "\n",
    "        print(f\"Finished {desired_landmark_name}, {desired_landmark_index+1}/{len(selected_EEG_10_20_indices)}\")\n",
    "\n",
    "else:\n",
    "    coordinates_coordinates_resolutions_MSE_array, coordinates_coordinates_resolutions_std_array = experiment_arrays_loader(regressor_name, experiment_number)\n",
    "    save_arrays = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
